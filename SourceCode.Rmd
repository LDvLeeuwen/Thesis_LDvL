---
title: "SourceCode"
output: pdf_document
date: "2023-04-30"
---

This is the Source code for the thesis project about predicting non-response in Scottish adolescents through multiverse modelling. The thesis itself, with detailed explanation about the methodological decision of the project, is at https://github.com/LDvLeeuwen/Thesis_LDvL. The data used in the project cannot be distributed, but can be retrieved from the website of the UK Data Service with an e-mail stating the goals with the data are non-commercial. 

A critical side note is about how many clusters the project analysed. Initially, the project analysed 4 clusters as the dependent variables, as the distance cutoff score mentioned later in the cluster analysis was 0.6. After feedback of superiors, this was considered too harsh, and a lower cutoff score of 0.2 was implemented. This resulted in 12 clusters, of which 10 were analysed.

This document is not meant to be knitted to html or pdf. For a clear overview of the results, the Thesis.pdf file should be consulted. Finally, it is highly encouraged to reproduce the results, but it should be noted that the models like Neural Networks and XGBoost Regression took a lot of time (~40 hours) to be completed. 

## Setup

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      tidy = TRUE)

# Setting a seed
set.seed(3334)
```

### Libraries

```{r Libraries, include=FALSE}
# General packages
library(sjlabelled)
library(haven)
library(dplyr)
library(tidyverse)
library(splitstackshape)
library(progress)
library(car)

# Visualisation
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(naniar)

# Processing, imputation and more
library(keras)
library(cluster)
library(tensorflow)
library(mice)
library(caret)
library(ggmice)
library(foreach)
library(parallel)
library(doParallel)

# Algorithmic models
library(randomForest)
library(e1071)
library(xgboost)
library(nnet)
library(neuralnet)
library(glmnet)
library(glmnetUtils)
library(ranger)
library(adabag)
```

### Loading in the data

```{r Loading in data}
raw_df <- read_dta('data/scottish.dta')
```

## Preprocessing

### Removal of irrelevant variables

#### Removal of routing questions

```{r Cleaning 1}
cleaned_df <- raw_df %>%                             
  select(where(~!any(. == -1, na.rm = TRUE))) %>%  # First, remove all columns that have routing questions
  mutate_all(~ifelse(. %in% c(-9,-99), NA, .)) %>% # Then, all true missing values were coded as -9 | -99
  select(where(~ mean(is.na(.)) < 0.8))            # Finally, remove some columns with more than 80% 
                                                   # missingness; explained below
```

#### Removal of redundant variables with correlation function

```{r Function - correlation}
calculate_high_correlations <- function(data, threshold) {
  # Calculate correlation matrix
  cor_matrix <- cor(data, use = 'pairwise.complete.obs')
  
  # Extract correlations greater than the threshold
  high_cor <- which(abs(cor_matrix) > threshold & cor_matrix != 1, arr.ind = TRUE)
  
  # Remove duplicate correlations
  high_cor <- high_cor[high_cor[,1] < high_cor[,2],]
  
  # Store column names in col1 and col2
  col1 <- names(data)[high_cor[,1]]
  col2 <- names(data)[high_cor[,2]]
  
  # Print results
  if (length(col2) > 0) {
    cat(paste0("The correlations greater than ", threshold, " are:\n"))
    for (i in 1:nrow(high_cor)) {
      var1 <- col1[i]
      var2 <- col2[i]
      ind1 <- high_cor[i,1]
      ind2 <- high_cor[i,2]
      cat(paste0(var1, " (", ind1, ") and ", var2, " (",ind2, "): ", cor_matrix[ind1, high_cor[i,2]], "\n"))
    }
  } else {
    cat("There are no correlations greater than ", threshold, ".\n")
  }
  
  # Find col1 columns not present in col2
  left_side <- setdiff(col1, col2)
  
  # Return the character vector
  return(unique(left_side))
}
```

```{r Cleaning 2}
# Calculating the pairs with correlation higher than 0.7
cor_result <- calculate_high_correlations(cleaned_df, 0.7)

# Removing the first half of those pairs
cleaned_df <- cleaned_df[, !(colnames(cleaned_df) %in% cor_result)]
```

#### Removal of redundant variables based on qualitative assessment

The reason to remove these character vectors are explained in detail in the thesis. 

```{r Cleaning 3}
# Get column names starting with '__', as they are redundant variables about name brands
redu1 <- colnames(cleaned_df)[grep("^__", colnames(cleaned_df))]

# Then remove all of the maknow paknow items, as all important information is stored in dadscore2
redu2 <- colnames(cleaned_df)[grep("^(ma|pa)know[1-5]x?$", colnames(cleaned_df))]

# Then remove some drug-related items that were coded differently but held similar information
redu3 <- colnames(cleaned_df)[grep("^dgof", colnames(cleaned_df))]

# Combine the column names with other redundant variables
redu_vector <- c("Refid", "mumscore", "dadscore", "smcost_bands","wwscore", "final_wt", "healthboard","numbrand", 
                 "locauth", "museum", "theatre", "SDQscore", "smokstat", "smokstat2", "subuse", "cgstat", 
                 redu1, redu2, redu3)

cleaned_df <- cleaned_df[, !(colnames(cleaned_df) %in% redu_vector)]
```

### Dependent variable selection

```{r Variable selection}
# Calculate the percent missing for each column
missing_percent <- colMeans(is.na(cleaned_df))

# Select the columns with more than 10% missingness
selected_cols <- names(missing_percent[missing_percent > 0.10])

# Then determine which variables are eligible
possible_df <- cleaned_df %>% 
  select(all_of(selected_cols)) 
```

#### Missingness pattern analysis

```{r Function - dummies}
dummy_maker <- function(data, variable_list) {
  dummy_names <- c()
  
  for (variable in variable_list) {
    
    # Add a suffix to make a distinction between the dummies and their original variables
    dummy_variable <- paste0(variable, "_D")
    
    # Determine whether data is missing (0 = missing, 1 = present)
    data[dummy_variable] <- ifelse(is.na(data[[variable]]), 0, 1)
    dummy_names <- c(dummy_names, dummy_variable) 
  }
  result <- list(data = data, dummy_names = dummy_names)
  return(result)
}
```

```{r Form dummies}
# Make dummies for each possible variable
dummy_result <- dummy_maker(possible_df, colnames(possible_df))
dummy_df <- dummy_result$data

# Remove all non-dummy variables
dummy_df <- dummy_df[, -grep("_D$", names(dummy_df), invert = TRUE)]

# Now, form distance matrix from the correlation matrix of the dummy_df
dist_m <- as.dist(1 - cor(dummy_df))
```

##### Cluster analysis

```{r Cluster analysis}
# Perform hierarchical clustering
hc <- hclust(dist_m, method = "average")

# Cut the clusters with threshold 0.2, meaning clusters can maximally be 80% correlated
clusters <- cutree(hc, h = 0.2) 
```

### Forming of target frames

```{r Function - extract_names}
extract_names <- function(data, cluster_numbers) {
  extracted_names <- list()
  
  for (cluster_number in cluster_numbers) {
    cluster_cols <- names(data[data == cluster_number])
    cleaned_cols <- gsub("_D$", "", cluster_cols)
    extracted_names[[as.character(cluster_number)]] <- cleaned_cols
  }
  
  return(extracted_names)
}

# Define the cluster numbers you want to extract
cluster_numbers <- c(2, 3, 4, 5, 6, 7, 8, 10, 11, 12)

# Extract the names from the selected clusters
extracted_clusters <- extract_names(clusters, cluster_numbers)
```

#### Make dummies for each target variable

```{r Function - create_target_frames}
create_target_frames <- function(original_data, column_vectors) {
  data_frames <- list()
  target_names <- list()
  
  for (i in 1:length(column_vectors)) {
    excluded_columns <- column_vectors[[i]]
    included_columns <- setdiff(colnames(original_data), excluded_columns)
    
    data <- original_data
    first_column <- excluded_columns[[1]]
    target_names[[i]] <- first_column
      
    # Create dummy variables for the first column with dummy function
    dummy_result <- dummy_maker(data, first_column)
    first_column <- dummy_result$dummy_names
      
    # Update the data frame with the dummy variables
    data <- dummy_result$data
      
    # Include the dummy variable name in the included columns
    included_columns <- c(included_columns, first_column)

    # Subset the data frame based on the included columns
    data_frames[[i]] <- data[, included_columns]

  }
  
  return(list(data_frames = data_frames, target_names = target_names))
}
```

The 10 target frames are as follows:

```{r Create target frames}
# Create the target frames with the extracted clusters
target_frames <- create_target_frames(cleaned_df, extracted_clusters)$data_frames

# Make a list for all the dummy names
target_names <- create_target_frames(cleaned_df, extracted_clusters)$target_names
target_names <- paste0(target_names, "_D")
```

#### Factorisation 

Bit of an odd step, but has to be done now for the imputation process later on,

```{r Function - factor_numeric_maker}
factor_numeric_maker <- function(data, numeric_vars) {
  
  to_transform <- names(data)[!names(data) %in% numeric_vars]
  
  new_data <- data.frame(matrix(ncol = ncol(data), nrow = nrow(data)))
  colnames(new_data) <- colnames(data)
  for (var in names(data)) {
    if (var %in% numeric_vars) {
      new_data[[var]] <- as.numeric(data[[var]])
    } else {
      new_data[[var]] <- as.factor(data[[var]])
    }
  }
  
  for (var in names(data)) {
    attr(new_data[[var]], "label") <- attr(data[[var]], "label")
  }
  
  return(new_data)
}
```

```{r Factorising data frame}
# Identify the numeric columns 
numeric_cols <- c("smcost", "numeffects")

for (i in seq_along(target_frames)) {
  target_frames[[i]] <- factor_numeric_maker(target_frames[[i]], numeric_cols)
}
```

### Imputation process

#### Imputation with mice

```{r Imputations with mice, cache = TRUE}
# List for storage of imputed target frames 
imputed_frames <- list()
matrix_list <- list()
logged_events <- list()

# Loop over each data frame
for (i in seq_along(target_frames)) {
  frame <- target_frames[[i]]  
  
  # Create predictor matrix using quickpred function
  predictor_matrix <- quickpred(frame[, -ncol(frame)], 
                                mincor = 0.2, 
                                minpuc = 0.2)
  
  # Impute missing values using mice
  imputations <- mice(frame, 
                       m = 1, 
                       maxit = 3, 
                       predictorMatrix = predictor_matrix,
                       nnet.MaxNWts = 12000)
  
  # Access the imputed data frames
  imputed_frame <- complete(imputations)
  
  # Store the imputed frames, imputations and matrices for later inspection
  imputed_frames[[i]] <- imputed_frame
  matrix_list[[i]] <- predictor_matrix
  logged_events[[i]] <- imputations$loggedEvents
  
}
```

##### Checking methods and logged events

There are no logged events present (in none of the imputations).

```{r Checking mice methods}
# Check method to see whether correct imputation methods were used
print(imputations$method)

# Check logged events
print(logged_events[[1]])
```

### Splitting the data set in train/test

```{r Function - split_data}
split_function <- function(data, target) {
  train_indices <- createDataPartition(data[,target],
                                       p = 0.7,
                                       list = FALSE)
  train_df <- data[train_indices, ]
  test_df <- data[-train_indices, ]
  
  return(list(train = train_df, test = test_df))
}
```

## Algorithmic Modelling

### Appending results (predictions & variable importance)

Performance of the models and variable importance were appended to a dataframe and exported for easy use in the Markdown file of the thesis. Else, the models would have to be run again (as knitr requires all models to be run at least once separately).

```{r Create results dataframe}
# This is for the results of each model
all_results_10C <- data.frame(Target = character(),
                          Model = character(),
                          Version = character(),
                          Sensitivity = numeric(),
                          Specificity = numeric(),
                          F1_Score = numeric(),
                          Balanced_Accuracy = numeric(),
                          Matthews_Correlation = numeric(),
                          stringsAsFactors = FALSE)
```

```{r Function - append_results}
append_results <- function(target, model, version, cm) {
 
  set.seed(3334)
  
  # Calculate Matthews correlation coefficient
  tp <- as.numeric(cm$table[2, 2])
  tn <- as.numeric(cm$table[1, 1])
  fp <- as.numeric(cm$table[1, 2])
  fn <- as.numeric(cm$table[2, 1])
 
  numerator <- (tp * tn) - (fp * fn)
  denominator <- sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
  
  # Handle dividing by zero/nan, as this was resulting in an error
  if (is.nan(numerator) || is.nan(denominator)) {
    matthews_corr <- 0  
  } else if (denominator == 0) {
    matthews_corr <- 0  
  } else {
    matthews_corr <- numerator / denominator
  }
 
  # Create new row with appended results
  new_row <- data.frame(Target = target,
                        Model = model,
                        Version = version,
                        Sensitivity = cm[["byClass"]][["Sensitivity"]],
                        Specificity = cm[["byClass"]][["Specificity"]],
                        F1_Score = cm[["byClass"]][["F1"]],
                        Balanced_Accuracy = cm[["byClass"]][["Balanced Accuracy"]],
                        Matthews_Correlation = matthews_corr,
                        stringsAsFactors = FALSE)
 
  # Append new row to all_results_10C data frame
  all_results_10C <<- rbind(all_results_10C, new_row)
}

```

```{r Create varImp dataframe}
# Create an empty dataframe to store the variable importance results
all_varImp_10C  <- data.frame(Model = character(),
                              Dependent = character(),
                              Variable = character(),
                              ImportanceScaled = numeric(),
                              ImportanceRaw = numeric(),
                              stringsAsFactors = FALSE)
```

```{r Function - append_variable_importance}
append_variable_importance <- function(models_list, name) {

  for (i in seq_along(models_list)) {

    model <- models_list[[i]]
    dependent <- target_names[[i]]
    varimp <- caret::varImp(object = model, scale = TRUE)
    varimp_raw <- caret::varImp(object = model, scale = FALSE)

    # Extract the variable names, scaled importance, and raw importance
    variables <- row.names(varimp$importance)
    importance_scaled <- varimp$importance
    importance_raw <- varimp_raw$importance

    # Change column names
    importance_scaled <- importance_scaled[variables, , drop = FALSE]
    importance_raw <- importance_raw[variables, , drop = FALSE]

    colnames(importance_scaled) <- c("ImportanceScaled")
    colnames(importance_raw) <- c("ImportanceRaw")

    # Create a temporary dataframe for the current model
    new_var <- data.frame(Model = name,
                          Variable = row.names(importance_scaled),
                          Dependent = dependent,
                          ImportanceScaled = importance_scaled,
                          ImportanceRaw = importance_raw,
                          stringsAsFactors = FALSE)

    # Append the temporary dataframe to the all_varImp dataframe
    all_varImp_10C <<- rbind(all_varImp_10C, new_var)
  }

}

```

### RandomForest

#### Data preparation

For the RandomForest algorithm, categorical variables should be coded as factors, which was done earlier.

```{r RF - Data prep}
# Create some empty lists for train/test sets
current_trainsets <- list() 
current_testsets <- list() 

# Loop over the data frames for training/test sets
for(i in seq_along(imputed_frames)) {
  
  current_frame <- imputed_frames[[i]]
  current_target <- target_names[[i]]
  
  # Split using split_function
  result <- split_function(current_frame, current_target)
  
  current_trainsets[[i]] <- result$train
  current_testsets[[i]] <- result$test
}
```

#### Base models

```{r RF - Base models}
RF0_models <- list()
set.seed(3334)

# Loop over the data frames and target variables
for (i in seq_along(current_trainsets)) {

  current_train <- current_trainsets[[i]]
  current_target <- target_names[i]

  x <- current_train[, !colnames(current_train) %in% current_target]
  y <- as.factor(current_train[[current_target]])
  
  # Apply base parameters
  model <- randomForest(x = x,
                        y = y,
                        data = current_train,
                        ntree = 500,
                        type = "classification")
  
  # Store the model in the list
  RF0_models[[i]] <- model
}
```

#### Tuned models (Parameter tuned)

```{r Function - balanced_calc}
# Make a new function to calculate based on balanced accuracy and not ROC/Normal accuracy
balanced_calc <- function(data, lev = NULL, model = NULL) {
  cm <- confusionMatrix(data = data$pred, reference = data$obs, positive = lev[1])
  acc <- cm$byClass["Balanced Accuracy"]
  return(acc)
}
```

Unfortunately, the code below resulted in an error. Weirdly enough, this error only occurred if the code happened in a for-loop; If done individually for each model, no error was given. 

```{r RF - Parameter tuned}
# RF1_models <- list()
# set.seed(3334)
# 
# # Loop over the data frames and target variables
# for (i in seq_along(current_trainsets)) {
# 
#   current_train <- current_trainsets[[i]]
#   current_target <- target_names[i]
# 
#   x <- current_train[, !colnames(current_train) %in% current_target]
#   y <- current_train[[current_target]]
#   
#   # Apply stratified k-folds
#   cvIndex <- createFolds(factor(y), 5, returnTrain = TRUE)
#   
#   # Define parameter grid
#   grid <- expand.grid(mtry = c(5, 10, 15),  
#                       splitrule = c("gini","extratrees"),
#                       min.node.size = c(1, 5, 10))
#   
#   # Let trainControl evaluate on Balanced Accuracy
#   ctrl <- trainControl(index = cvIndex,
#                        method = "cv",
#                        classProbs = TRUE,
#                        summaryFunction = balanced_calc,
#                        verboseIter = FALSE,
#                        allowParallel = TRUE)
# 
#   model <- caret::train(x, 
#                         make.names(y),
#                         method = "ranger",
#                         trControl = ctrl,
#                         tuneGrid = grid,
#                         metric = "Balanced Accuracy",
#                         importance = "permutation")
#   
#   # Store the model in the list
#   RF1_models[[i]] <- model
# 
# }
```

This code does work, for some reason. This ran individually for each model.

```{r RF - Parameter tuned (without crashing)}
set.seed(3334)


# Loop over the data frames and target variables
current_train <- current_trainsets[[10]]
current_target <- target_names[10]

x <- current_train[, !colnames(current_train) %in% current_target]
y <- current_train[[current_target]]
  
# Apply stratified k-folds
cvIndex <- createFolds(factor(y), 5, returnTrain = TRUE)
  
# Define parameter grid
grid <- expand.grid(mtry = c(5, 10, 15),  
                      splitrule = c("gini","extratrees"),
                      min.node.size = c(1, 5, 10))
  
# Let trainControl evaluate on Balanced Accuracy
ctrl <- trainControl(index = cvIndex,
                       method = "cv",
                       classProbs = TRUE,
                       summaryFunction = balanced_calc,
                       verboseIter = FALSE,
                       allowParallel = FALSE)

model <- caret::train(x, 
                        make.names(y),
                        method = "ranger",
                        trControl = ctrl,
                        tuneGrid = grid,
                        metric = "Balanced Accuracy",
                        importance = "permutation")
  
  # Store the model in the list
RF1_models[[10]] <- model


```

#### Tuned models (Parameter tuned + Class weights)

```{r RF - Weights}
RF_weights <- list()

for (i in seq_along(current_trainsets)) {
  current_train <- current_trainsets[[i]]
  current_target <- target_names[i]
 
  # Calculate class frequencies
  class_freq <- table(current_train[, current_target])
 
  # Calculate class weights inversely
  weights <- ifelse(current_train[, current_target] == 0,
                    1 / class_freq[1],
                    1 / class_freq[2])
 
  # Create vector of case weights
  case_weights <- weights[current_train[, current_target]]
 
  RF_weights[[i]] <- weights
}
```

```{r RF - Parameter tuned + class weigths}
RF2_models <- list()
set.seed(3334)

# Loop over the data frames and target variables
for (i in seq_along(current_trainsets)) {

  current_train <- current_trainsets[[i]]
  current_target <- target_names[i]

  x <- current_train[, !colnames(current_train) %in% current_target]
  y <- current_train[[current_target]]
  
  # Apply stratified k-folds
  cvIndex <- createFolds(factor(y), 5, returnTrain = TRUE)
  
  # Define parameter grid
  grid <- expand.grid(mtry = c(5, 10, 15),  
                      splitrule = c("gini","extratrees"),
                      min.node.size = c(1, 5, 10))
  
  # Let trainControl evaluate on Balanced Accuracy
  ctrl <- trainControl(index = cvIndex,
                       method = "cv",
                       classProbs = TRUE,
                       summaryFunction = balanced_calc,
                       verboseIter = FALSE)
  
  # Add the weights
  model <- caret::train(x, 
                        make.names(y),
                        method = "ranger",
                        trControl = ctrl,
                        tuneGrid = grid,
                        metric = "Balanced Accuracy",
                        weights = RF_weights[[i]])
  
  # Store the model in the list
  RF2_models[[i]] <- model

}
```

#### Tuned models (Parameter tuned + Class weights + Threshold tuned)

```{r Function - Threshold tuning}
threshold_tuning <- function(models, trainsets, label) {
  optimal_thresholds <- list()
  max_balanced_accs <- list()
  thresholds <- seq(0.05, 0.5, 0.05)

  for (i in seq_along(models)) {
    model <- models[[i]]
    current_train <- trainsets[[i]]
    current_target <- current_train[[target_names[i]]]

    predicted_probs <- predict(model, type = "prob")

    evaluation <- data.frame(
      Threshold = numeric(),
      "Balanced Accuracy" = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Make predictions on the training set
    for (threshold in thresholds) {
      predicted_labels <- factor(ifelse(predicted_probs[, label] >= threshold, 0, 1), levels = c(0, 1))
      eval_metrics <- caret::confusionMatrix(predicted_labels, as.factor(current_target))
      balanced_acc <- eval_metrics$byClass["Balanced Accuracy"]
      new_row <- data.frame(
        "Threshold" = threshold,
        "BalancedAccuracy" = balanced_acc
      )
      evaluation <- rbind(evaluation, new_row)
    }

    max_balanced_acc <- max(evaluation[, "BalancedAccuracy"])

    # Get the last optimal threshold with the maximum balanced accuracy
    optimal_threshold <- tail(
      evaluation$Threshold[evaluation$BalancedAccuracy == max_balanced_acc],
      n = 1
    )

    optimal_thresholds[[i]] <- optimal_threshold
    max_balanced_accs[[i]] <- max_balanced_acc

    cat("Model", i, ":\n")
    cat("Optimal Threshold:", optimal_threshold, "\n")
    cat("Balanced Accuracy:", max_balanced_acc, "\n\n")
  }

  return(list(optimal_thresholds = optimal_thresholds, max_balanced_accs = max_balanced_accs))
}
```

```{r RF Parameter tuned + class weights + threshold tuning}
RF_thresholds <- threshold_tuning(RF2_models, current_trainsets, "X0")
```

#### Results 

##### Predictions

```{r RF - Base models results}
RF0_preds <- list()

for (i in seq_along(RF0_models)) {
  
  model <- RF0_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- current_test[[target_names[i]]]
  
  RF0_preds[[i]] <- predict(model, current_test)
  
  cm <- confusionMatrix(data = factor(RF0_preds[[i]], levels = c(0, 1)),
                        reference = current_target)
  
  # Append the results to the all_results df
  append_results(target_names[i], "Random Forest", "Base", cm)
}
```

```{r RF - Parameter tuned results}
RF1_preds <- list()

for (i in seq_along(RF1_models)) {
  
  model <- RF1_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- current_test[[target_names[i]]]
  
  # RF predictions give levels X0 and X1, so convert them
  RF1_preds[[i]] <- ifelse(predict(model, current_test) == "X0", 0, 1)

  cm <- confusionMatrix(data = factor(RF1_preds[[i]], levels = c(0, 1)),
                        reference = current_target)
  
  # Append the results to the all_results df
  append_results(target_names[i], "Random Forest", "Parameter tuned", cm)
}
```

```{r RF - Parameter tuned + class_weights results}
RF2_preds <- list()

for (i in seq_along(RF2_models)) {
  
  model <- RF2_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- current_test[[target_names[i]]]
  
  # RF predictions give levels X0 and X1, so convert them
  RF2_preds[[i]] <- ifelse(predict(model, current_test) == "X0", 0, 1)

  cm <- confusionMatrix(data = factor(RF2_preds[[i]], levels = c(0, 1)),
                        reference = current_target)
  
  # Append the results to the all_results df
  append_results(target_names[i], "Random Forest", "Parameter tuned + Class weights", cm)
}
```

```{r RF - Parameter tuned + class weights + threshold tuning results}
RF3_preds <- list()

for (i in seq_along(RF2_models)) {
  
  model <- RF2_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- current_test[[target_names[i]]]
  
  # Check if optimal threshold is something else than 0.5
  if (RF_thresholds$optimal_thresholds[[i]] != 0.5) {
    
    # Make probability predictions
    RF3_preds[[i]] <- predict(model, current_test, type = "prob")
    
    # Apply different thresholds
    binary_preds <- ifelse(RF3_preds[[i]][, "X0"] >= RF_thresholds$optimal_thresholds[[i]], 0, 1)
    
    cm <- confusionMatrix(data = factor(binary_preds, levels = c(0, 1)),
                          reference = current_target)
  
  # Append the results to the all_results df
  append_results(target_names[i], "Random Forest", "Parameter tuned + Class weights + Threshold tuned", cm)
  }
}
```

##### Variable importance

```{r RF - Variable importance}
append_variable_importance(RF1_models, "Random Forest")
```

### Neural Networks 

#### Data preparation 

For the Neural Networks algorithm, categorical variables had to be one-hot encoded with fastDummies::dummy_cols

```{r NN - Data prep}
onehot_frames <- list()

for (i in seq_along(imputed_frames)) {
  
  # Scale numeric variables if present
  current_frame <- imputed_frames[[i]]
  current_frame <- current_frame %>% 
    mutate_at(c("smcost","numeffects"), ~(scale(.) %>% as.vector))
  
  # Then, perform one-hot encoding for the factor variables
  encoded_frame <- fastDummies::dummy_cols(current_frame,
                                           select_columns = names(current_frame)[!(names(current_frame) %in%
                                        c(numeric_cols, target_names))],
                                        remove_first_dummy = TRUE,
                                        remove_selected_columns = TRUE)
  
  # Assign the updated data frame back to the list
  onehot_frames[[i]] <- encoded_frame
}
```

```{r NN - Data prep 2}
# Set different seed
set.seed(222)

# Create some empty lists for train/test sets
current_trainsets <- list() 
current_testsets <- list() 

# Loop over the data frames for training/test sets
for(i in seq_along(onehot_frames)) {
  
  current_frame <- onehot_frames[[i]]
  current_target <- target_names[[i]]
  
  # Split using split_function
  result <- split_function(current_frame, current_target)
  
  current_trainsets[[i]] <- result$train
  current_testsets[[i]] <- result$test
}
```

#### Base models

```{r NN - Base models}
NN0_models <- list()
set.seed(3334)

# Loop over the data frames and target variables
for (i in seq_along(current_trainsets)) {

  current_train <- current_trainsets[[i]]
  current_target <- target_names[i]

  x <- current_train[, !colnames(current_train) %in% current_target]
  y <- as.numeric(current_train[[current_target]])
  
  # Make some baseline models
  model <- caret::train(x = x,
                 y = make.names(y),
                 method = "nnet",
                 trControl = trainControl(method = "none"),
                 trace = FALSE,
                 MaxNWts = 5000)

  # Store the model in the list
  NN0_models[[i]] <- model
}
```

#### Tuned models (Parameter tuned)

```{r NN - Parameter tuned}
NN1_models <- list()
set.seed(3334)

# Loop over the data frames and target variables
for (i in seq_along(current_trainsets)) {

  current_train <- current_trainsets[[i]]
  current_target <- target_names[i]

  x <- current_train[, !colnames(current_train) %in% current_target]
  y <- as.numeric(current_train[[current_target]])
  
  # Apply stratified k-folds
  cvIndex <- createFolds(factor(y), 5, returnTrain = TRUE)
  
  # Define parameter grid
  grid <- expand.grid(size = c(5, 10, 20, 50),  
                      decay = c(0, 0.001, 0.01, 0.1))
  
  # Let trainControl evaluate on Balanced Accuracy
  ctrl <- trainControl(index = cvIndex,
                       method = "cv",
                       classProbs = TRUE,
                       summaryFunction = balanced_calc,
                       verboseIter = FALSE,
                       allowParallel = TRUE)

  model <- caret::train(x,
                        make.names(y),
                        method = "nnet",
                        trControl = ctrl,
                        tuneGrid = grid,
                        metric = "Balanced Accuracy",
                        MaxNWts = 30000,
                        trace = FALSE)
  
  # Store the model in the list
  NN1_models[[i]] <- model
}
```

#### Tuned models (Parameter tuned + Class weights)

```{r NN - Weights}
NN_weights <- list()

for (i in seq_along(current_trainsets)) {
  current_train <- current_trainsets[[i]]
  current_target <- target_names[i]
 
  # Calculate class frequencies
  class_freq <- table(current_train[, current_target])
 
  # Calculate class weights inversely
  weights <- ifelse(current_train[, current_target] == 0,
                    1 / class_freq[1],
                    1 / class_freq[2])
 
  # Create vector of case weights
  case_weights <- weights[current_train[, current_target]]
 
  NN_weights[[i]] <- weights
}
```

```{r NN - Parameter tuned + Class weights}
NN2_models <- list()
set.seed(3334)

# Loop over the data frames and target variables
for (i in seq_along(current_trainsets)) {

  current_train <- current_trainsets[[i]]
  current_target <- target_names[i]

  x <- current_train[, !colnames(current_train) %in% current_target]
  y <- as.numeric(current_train[[current_target]])
  
  # Apply stratified k-folds
  cvIndex <- createFolds(factor(y), 5, returnTrain = TRUE)
  
  # Define parameter grid
  grid <- expand.grid(size = c(5, 10, 20, 50),  
                      decay = c(0, 0.001, 0.01, 0.1))
  
  # Let trainControl evaluate on Balanced Accuracy
  ctrl <- trainControl(index = cvIndex,
                       method = "cv",
                       classProbs = TRUE,
                       summaryFunction = balanced_calc,
                       verboseIter = FALSE,
                       allowParallel = TRUE)

  model <- caret::train(x,
                        make.names(y),
                        method = "nnet",
                        trControl = ctrl,
                        tuneGrid = grid,
                        metric = "Balanced Accuracy",
                        MaxNWts = 30000,
                        weights = NN_weights[[i]],
                        trace = FALSE)
  
  # Store the model in the list
  NN2_models[[i]] <- model
}
```

#### Tuned models (Parameter tuned + Class weights + Threshold tuned)

```{r NN - Parameter tuned + Class weights + Threshold tuning}
NN_thresholds <- threshold_tuning(NN2_models, current_trainsets, "X1")
```

#### Results 

##### Predictions

```{r NN - Base models results, warning = FALSE}
NN0_preds <- list()

for (i in seq_along(NN0_models)) {
  
  model <- NN0_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- current_test[[target_names[i]]]
  
  # Predictions have levels X1 and X2, so convert them to 0 and 1 respectively
  NN0_preds[[i]] <- ifelse(predict(model, current_test) == "X1", 0, 1)
    
  cm <- confusionMatrix(data = factor(NN0_preds[[i]], levels = c(0, 1)),
                        reference = current_target)
  
  # Append the results to the all_results df
  append_results(target_names[i], "Neural Networks", "Base", cm)
}
```

```{r NN - Parameter tuned results}
NN1_preds <- list()

for (i in seq_along(NN1_models)) {
  
  model <- NN1_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- current_test[[target_names[i]]]
  
  # Predictions have levels X1 and X2, so convert them to 0 and 1 respectively
  NN1_preds[[i]] <- ifelse(predict(model, current_test) == "X1", 0, 1)

  cm <- confusionMatrix(data = as.factor(NN1_preds[[i]]),
                        reference = current_target)
  
  # Append the results to the all_results df
  append_results(target_names[i], "Neural Networks", "Parameter tuned", cm)
}
```

```{r NN - Parameter tuned + class weights results}
NN2_preds <- list()

for (i in seq_along(NN2_models)) {
  
  model <- NN2_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- current_test[[target_names[i]]]
  
  # Predictions have levels X1 and X2, so convert them to 0 and 1 respectively
  NN1_preds[[i]] <- ifelse(predict(model, current_test) == "X1", 0, 1)

  cm <- confusionMatrix(data = as.factor(NN1_preds[[i]]),
                        reference = current_target)
  
  # Append the results to the all_results df
  append_results(target_names[i], "Neural Networks", "Parameter tuned + Class weights", cm)
}
```

```{r NN - Parameter tuned + class weights + threshold tuning results}
NN3_preds <- list()

for (i in seq_along(NN2_models)) {
  
  model <- NN2_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- current_test[[target_names[i]]]
  
  # Check if optimal threshold is something else than 0.5
  if (NN_thresholds$optimal_thresholds[[i]] != 0.5) {
    
    # Make probability predictions
    NN3_preds[[i]] <- predict(model, current_test, type = "prob")
    
    # Apply different thresholds
    binary_preds <- ifelse(NN3_preds[[i]][, "X1"] >= NN_thresholds$optimal_thresholds[[i]], 0, 1)
    
    cm <- confusionMatrix(data = as.factor(binary_preds),
                          reference = current_target)
  
  # Append the results to the all_results df
  append_results(target_names[i], "Neural Networks", "Parameter tuned + Class weights + Threshold tuned", cm)
  }
}
```

##### Variable importance

```{r NN - Variable importance}
append_variable_importance(NN2_models, "Neural Networks")
```

### XGBoost Regression

#### Data preparation

For the XGBoost algorithm, categorical variables had to be one-hot encoded with fastDummies::dummy_cols

```{r XG - Data prep}
# Set different seed
set.seed(333)

# Create some empty lists for train/test sets
current_trainsets <- list() 
current_testsets <- list() 

# Loop over the data frames for training/test sets
for(i in seq_along(onehot_frames)) {
  
  current_frame <- onehot_frames[[i]]
  current_target <- target_names[[i]]
  
  # Split using split_function
  result <- split_function(current_frame, current_target)
  
  current_trainsets[[i]] <- result$train
  current_testsets[[i]] <- result$test
}
```

#### Base models

```{r XG - Base models}
XG0_models <- list()
set.seed(3334)

# Iterate over the data frames and target variables
for (i in seq_along(current_trainsets)) {
  
  current_train <- current_trainsets[[i]]
  current_target <- target_names[i]
  
  # Preprocess the target variable as binary
  current_train[[current_target]] <- ifelse(current_train[[current_target]] == 0, 0, 1)

  # Prepare the data matrix
  dtrain <- xgb.DMatrix(data = as.matrix(current_train[, !colnames(current_train) %in% current_target]), 
                        label = current_train[[current_target]])
  
  model <- xgb.train(data = dtrain, 
                     nrounds = 100)
  # Store model
  XG0_models[[i]] <- model
}
```

#### Tuned models (Parameter tuned)

```{r XG - Parameter tuned}
XG1_models <- list()
set.seed(3334)

# Iterate over the data frames and target variables
for (i in seq_along(current_trainsets)) {
  
  current_train <- current_trainsets[[i]]
  current_target <- target_names[i]
  
  x <- current_train[, !colnames(current_train) %in% current_target]
  y <- as.factor(current_train[[current_target]])
  
  # Apply stratified k-folds
  cvIndex <- createFolds(factor(y), 5, returnTrain = TRUE)
  
  # Let trainControl evaluate on Balanced Accuracy
  ctrl <- trainControl(index = cvIndex,
                       method = "cv",
                       classProbs = TRUE,
                       summaryFunction = balanced_calc,
                       verboseIter = FALSE,
                       allowParallel = TRUE) 
  
  # Define parameter grid
  grid <- expand.grid(nrounds = c(100, 200, 300),      
                      max_depth = c(3, 6, 9),     
                      eta = c(0.1, 0.3, 0.5),     
                      gamma = c(0, 0.1, 0.2),
                      colsample_bytree = c(0.6, 0.8, 1),
                      min_child_weight = c(1, 3, 5),
                      subsample = c(0.6, 0.8, 1))
  
  model <- caret::train(x, 
                        make.names(y),
                        method = "xgbTree",           
                        trControl = ctrl,            
                        tuneGrid = grid,
                        metric = "Balanced Accuracy",
                        verbosity = 0)
  # Store model
  XG1_models[[i]] <- model
}
```

#### Tuned models (Parameter tuned + Class weights) (Not working)

The code below unfortunately did not work. No solution was found, and thus the class weights version of the XGBoost Regression model was not implemented.

```{r XG - Weights}
# XG_weights <- list()
# 
# for (i in seq_along(current_trainsets)) {
#   current_train <- current_trainsets[[i]]
#   current_target <- target_names[i]
#  
#   # Calculate class frequencies
#   class_freq <- table(current_train[, current_target])
#  
#   # Calculate class weights inversely
#   weights <- ifelse(current_train[, current_target] == 0,
#                     1 / class_freq[1],
#                     1 / class_freq[2])
#  
#   # Create vector of case weights
#   case_weights <- weights[current_train[, current_target]]
#  
#   XG_weights[[i]] <- weights
# }
```

```{r XG - Parameter tuned + Class weights NOT WORKING}
# XG2_models <- list()
# set.seed(3334)
# 
# # Iterate over the data frames and target variables
# for (i in seq_along(current_trainsets)) {
#   
#   current_train <- current_trainsets[[i]]
#   current_target <- target_names[i]
#   
#   x <- current_train[, !colnames(current_train) %in% current_target]
#   y <- as.factor(current_train[[current_target]])
#   
#   # Apply stratified k-folds
#   cvIndex <- createFolds(factor(y), 5, returnTrain = TRUE)
#   
#   # Let trainControl evaluate on Balanced Accuracy
#   ctrl <- trainControl(index = cvIndex,
#                        method = "cv",
#                        classProbs = TRUE,
#                        summaryFunction = balanced_calc,
#                        verboseIter = FALSE,
#                        allowParallel = TRUE) 
#   
#   # Define parameter grid
#   grid <- expand.grid(nrounds = c(100, 200, 300),      
#                       max_depth = c(3, 6, 9),     
#                       eta = c(0.1, 0.3, 0.5),     
#                       gamma = c(0, 0.1, 0.2),
#                       colsample_bytree = c(0.6, 0.8, 1),
#                       min_child_weight = c(1, 3, 5),
#                       subsample = c(0.6, 0.8, 1))
#   
#   model <- caret::train(x, 
#                         make.names(y),
#                         method = "xgbTree",           
#                         trControl = ctrl,            
#                         tuneGrid = grid,
#                         metric = "Balanced Accuracy",
#                         weights = XG_weights[[i]],
#                         verbosity = 0)
#   # Store model
#   XG2_models[[i]] <- model
#   
#   print("Check")
# }
```

#### Tuned models (Parameter tuned + Threshold tuned)

```{r XG - Parameter tuned + Treshold tuned}
XG_thresholds <- threshold_tuning(XG1_models, current_trainsets, "X0")
```

#### Results 

##### Predictions 

```{r XG - Base models results}
XG0_preds <- list()

for (i in seq_along(XG0_models)) {
  
  model <- XG0_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- target_names[i]
  current_test[[current_target]] <- ifelse(current_test[[current_target]] == 0, 0, 1)
  
  # Prepare the data matrix
  dtest <- xgb.DMatrix(data = as.matrix(current_test[, !colnames(current_test) %in% current_target]), 
                       label = current_test[[current_target]])

  XG0_preds[[i]] <- predict(model, dtest)

  # Convert externalptr object to numeric vector
  XG0_preds[[i]] <- as.numeric(XG0_preds[[i]])
  
  # Convert predicted values to binary labels
  XG0_preds[[i]] <- factor(ifelse(XG0_preds[[i]] > 0.5, 1, 0), levels = c(0, 1))
    
  cm <- confusionMatrix(data = as.factor(XG0_preds[[i]]),
                        reference = as.factor(current_test[[current_target]]))
  
  # Append the results to the all_results df
  append_results(target_names[i], "XGBoost Regression", "Base", cm)
}
```

```{r XG - Parameter tuned results}
XG1_preds <- list()

for (i in seq_along(XG1_models)) {
  
  model <- XG1_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- target_names[i]
  current_test[[current_target]] <- ifelse(current_test[[current_target]] == 0, 0, 1)
  
  # Predictions have levels X0 and X1, so convert them
  XG1_preds[[i]] <- factor(ifelse(predict(model, current_test) == "X0", 0, 1), levels = c(0,1))

  cm <- confusionMatrix(data = XG1_preds[[i]],
                        reference = as.factor(current_test[[current_target]]))
  
  # Append the results to the all_results df
  append_results(target_names[i], "XGBoost Regression", "Parameter tuned", cm)
}
```

As this version of the model did not work, no predictions could be made.

```{r XG - Parameter tuned + class weights results}
# XG2_preds <- list()
# 
# for (i in seq_along(XG2_models)) {
#   
#   model <- XG2_models[[i]]
#   current_test <- current_testsets[[i]]
#   current_target <- target_names[i]
#   current_test[[current_target]] <- ifelse(current_test[[current_target]] == 0, 0, 1)
#   
#   # Predictions have levels X0 and X1, so convert them
#   XG2_preds[[i]] <- factor(ifelse(predict(model, current_test) == "X0", 0, 1), levels = c(0,1))
# 
#   cm <- confusionMatrix(data = XG2_preds[[i]],
#                         reference = as.factor(current_test[[current_target]]))
#   
#   # Append the results to the all_results df
#   append_results(target_names[i], "XGBoost Regression", "Parameter tuned + Class weights", cm)
# }
```

```{r XG - Parameter tuned + threshold tuning results}
XG3_preds <- list()

for (i in seq_along(XG1_models)) {
  
  model <- XG1_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- current_test[[target_names[i]]]
  
  # Check if optimal threshold is something else than 0.5
  if (XG_thresholds$optimal_thresholds[[i]] != 0.5) {
    
    # Make probability predictions
    XG3_preds[[i]] <- predict(model, current_test, type = "prob")
    
    # Apply different thresholds
    binary_preds <- ifelse(XG3_preds[[i]][, "X0"] >= XG_thresholds$optimal_thresholds[[i]], 0, 1)
    
    cm <- confusionMatrix(data = as.factor(binary_preds),
                          reference = current_target)
  
  # Append the results to the all_results df
  append_results(target_names[i], "XGBoost Regression", "Parameter tuned + Threshold tuned", cm)
  }
}
```

##### Variable importance 

```{r XG - Variable importance}
append_variable_importance(XG1_models, "XGBoost Regression")
```

### AdaBoost 

#### Data preparation

For the Adaptive Boosting algorithm, categorical variables had to be coded as factors, which was done earlier.

```{r AB - Data Prep}
# Set different seed
set.seed(667)

# Create some empty lists for train/test sets
current_trainsets <- list() 
current_testsets <- list() 

# Loop over the data frames for training/test sets
for(i in seq_along(imputed_frames)) {
  
  current_frame <- imputed_frames[[i]]
  current_target <- target_names[[i]]
  
  # Split using split_function
  result <- split_function(current_frame, current_target)
  
  current_trainsets[[i]] <- result$train
  current_testsets[[i]] <- result$test
}
```

#### Base models

```{r AB - Base models}
AB0_models <- list()
set.seed(3334)

# Loop over the data frames and target variables
for (i in seq_along(current_trainsets)) {

  current_train <- current_trainsets[[i]]
  current_target <- target_names[i]

  formula_str <- paste(current_target, "~ .")
  
  model <- boosting(formula = as.formula(formula_str),
                        data = current_train,
                        boos = TRUE,
                        mfinal = 100)
  
  # Store the model in the list
  AB0_models[[i]] <- model
}
```

#### Tuned (Parameter tuned)

```{r AB - Parameter tuned}
AB1_models <- list()
set.seed(3334)

for (i in seq_along(current_trainsets)) {
  
  current_train <- current_trainsets[[i]]
  current_target <- target_names[i]
  
  x <- current_train[, !colnames(current_train) %in% current_target]
  y <- current_train[[current_target]]
  
  # Apply stratified k-folds
  cvIndex <- createFolds(factor(y), 5, returnTrain = TRUE)
  
  # Define parameter grid
  grid <- expand.grid(mfinal = c(50, 100, 150),
                      maxdepth = c(2, 3, 4),
                      coeflearn = c("Freund", "Breiman", "Zhu"))
  
  # Let trainControl evaluate on Balanced Accuracy
  ctrl <- trainControl(index = cvIndex,
                       method = "cv",
                       classProbs = TRUE,
                       summaryFunction = balanced_calc,
                       allowParallel = TRUE)
  
  model <- caret::train(x, 
                        make.names(y),
                        method = "AdaBoost.M1",
                        trControl = ctrl,
                        tuneGrid = grid,
                        metric = "Balanced Accuracy")
  
  AB1_models[[i]] <- model
  elapsed_time <- difftime(Sys.time(), start_time)
  cat("Elapsed time:", round(elapsed_time, 2), "hours\n")
}
```

#### Tuned (Parameter tuned + Threshold tuned)

```{r AB - Parameter tuned + Threshold tuned}
AB_thresholds <- threshold_tuning(AB1_models, current_trainsets, "X0")
```

#### Results

##### Predictions

```{r AB - Base models results}
AB0_preds <- list()

for (i in seq_along(AB0_models)) {
  
  model <- AB0_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- current_test[[target_names[i]]]
  
  # Make predictions
  AB0_preds[[i]] <- predict(model, current_test)

  cm <- confusionMatrix(data = as.factor(AB0_preds[[i]]$class),
                        reference = as.factor(current_target))
  
  # Append the results to the all_results df
  append_results(target_names[i], "Adaptive Boosting", "Base", cm)
}
```

```{r AB - Parameter tuned results}
AB1_preds <- list()

for (i in seq_along(AB1_models)) {
  
  model <- AB1_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- current_test[[target_names[i]]]
  
  # Predictions have levels X0 and X1, so convert them to 0 and 1 respectively
  AB1_preds[[i]] <- ifelse(predict(model, current_test) == "X0", 0, 1)
  
  cm <- confusionMatrix(data = as.factor(AB1_preds[[i]]),
                        reference = current_target)
  
  # Append the results to the all_results df
  append_results(target_names[i], "Adaptive Boosting", "Parameter tuned", cm)
}
```

```{r AB - Parameter tuned + threshold tuning results}
AB2_preds <- list()

for (i in seq_along(AB1_models)) {
  
  model <- AB1_models[[i]]
  current_test <- current_testsets[[i]]
  current_target <- current_test[[target_names[i]]]
  
  # Check if optimal threshold is something else than 0.5
  if (AB_thresholds$optimal_thresholds[[i]] != 0.5) {
    
    # Make probability predictions
    AB2_preds[[i]] <- predict(model, current_test, type = "prob")
    
    # Apply different thresholds
    binary_preds <- ifelse(AB2_preds[[i]][, "X0"] >= AB_thresholds$optimal_thresholds[[i]], 0, 1)
    
    cm <- confusionMatrix(data = as.factor(binary_preds),
                          reference = current_target)
  
  # Append the results to the all_results df
  append_results(target_names[i], "Adaptive Boosting", "Parameter tuned + Threshold tuned", cm)
  }
}
```

##### Variable importance

```{r AB - Variable importance}
append_variable_importance(AB1_models, "Adaptive Boosting")
```

## Closing

```{r Export results df}
# Exporting the dataframe to a CSV file for the Markdown 
write.csv(all_results_10C, 
          "data/all_results.csv", 
          row.names = FALSE)
```

How the aggregation of variable importance for one-hot encoded variables was performed is explained in the Thesis pdf.

```{r Aggregate variable importance}
all_varImp_10C <- aggregate_var_importance(all_varImp_10C, "XGBoost Regression")
all_varImp_10C <- aggregate_var_importance(all_varImp_10C, "Neural Networks")
```

```{r Export varImp df}
# Remove the row names
row.names(all_varImp_10C) <- NULL

# Remove the dummy indicator
all_varImp_10C$Dependent <- gsub("_D", "", all_varImp_10C$Dependent)

# Export varImp df
write.csv(all_varImp_10C, 
          "data/all_varImp_10C.csv", 
          row.names = FALSE)
```

```{r Save workspace}
# Saving workspace image
save.image(file = "10Clusters.RData")
```

Fin

