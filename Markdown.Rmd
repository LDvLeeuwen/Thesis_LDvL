---
output:
   pdf_document:
     toc_depth: 4
     keep_tex: TRUE
     number_sections: TRUE
     latex_engine: xelatex
     fig_caption: yes
     extra_dependencies: ["float"]
documentclass: article
urlcolor: blue
linkcolor: black
header-includes:
  - \usepackage{multirow}
  - \usepackage{booktabs}
  - \usepackage{caption}
  - \usepackage{sectsty}
  - \usepackage{setspace}
  - \usepackage{geometry}
  - \usepackage{titlesec}
  - \usepackage{titletoc}
  - \usepackage{floatrow}
  - \usepackage{placeins}
  - \usepackage{subcaption}
  - \usepackage{colortbl}
  - \usepackage{longtable}
  - \usepackage{array}
  - \floatsetup[figure]{capposition=top}
  - \floatsetup[table]{capposition=top}
  - \setcounter{tocdepth}{4}
  - \setcounter{secnumdepth}{4}
---

<!-- I know this document is very inefficient. My apologies. -->



<!-- Frontpage -->

\newgeometry{top=25mm,bottom=25mm,inner=25mm,outer=25mm}

\thispagestyle{empty}

\begin{titlepage}
\centering

{\scshape \Large
Utrecht University\\
}
\vspace{4mm}
{\Large
Department of Information and Computing Science\\
}
{\Large
Department of Methodology and Statistics}
\vspace{8mm}
\hrule
\vspace{4mm}
\begin{spacing}{1.8}
{\large\textbf{
Thesis Project MSc Applied Data Science 
}}
\end{spacing}
\vspace{22mm}

\begin{spacing}{2.0}
{\Large \bf A Lost Cause? Determining what variables are associated with non-response in adolescents through multiverse modelling}\\
\end{spacing}

\vfill

\begin{center}
\begin{minipage}[t]{0.5\textwidth}
\large
\begin{center}
\textbf{First examiner:}\\
K.M. Lang
\end{center}
\vspace{5mm}
\begin{center}
\textbf{Second examiner:}\\
M.J.L.F. Cruyff
\end{center}
\vspace{5mm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}\raggedleft
\large
\begin{center}
\textbf{Candidate:}\\
L.S. Docters van Leeuwen
\end{center}
\vspace{5mm}
\begin{center}
\textbf{Student number:}\\
6411282
\end{center}
\end{minipage}
\end{center}

\vspace{24mm}

\begin{center}
\large \today
\end{center}

\end{titlepage}
\restoregeometry

<!-- Abstract -->

\thispagestyle{empty}
\begin{abstract}


This project aimed to identify variables that consistently predict item non-response in Scottish adolescents using multiverse modeling. To implement multiverse modelling, four different state of the art algorithms were employed, along with various tuned versions. The focus of the project was on individual variable importance, although model performance was essential as well.

The results indicated that all algorithms (except for the Random Forest) performed reasonably well after tuning, depending on the target variable. The Random Forest models exhibited poor performance across all target variables and were excluded from the overall variable importance analysis. Three significant findings emerged from the study. Firstly, variables from a mental health questionnaire showed associations with missingness in multiple dependent variables. Notably, variables related to emotions, fear, prosocial behavior, and hyperactivity were identified as important for multiple targets. Secondly, missingness on the questionnaire itself was associated with variables related to alcohol and drug use. Lastly, missingness in the variable concerning parental supervision was strongly associated with whether the adolescent was likely tot talk to their father/carer if they had concerns. 

The project had a few limitations, including some technical shortcomings. The hyperparameter optimisation process could have been more comprehensive, and the preprocessing steps were considered too harsh in retrospect. Recommendations for future research included different options to account for imbalanced data, which is ever-present in survey data. Ethical concerns about interpreting machine learning results and what research in the field on response mechanisms should emphasise were discussed as well. 

\end{abstract}


<!-- Table of Contents -->

\clearpage
\thispagestyle{empty}
\renewcommand{\contentsname}{\LARGE\bfseries Table of Contents}

\begin{spacing}{1.2}
\tableofcontents
\end{spacing}

<!-- Data backend -->

<!-- This data is needed for the visualisation. The results of the models are derived from a separate csv file. -->

```{r Setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE, 
                      tidy = TRUE, 
                      message = FALSE, 
                      warning = FALSE,
                      cache = TRUE)

# Setting a seed
set.seed(3334)
```

```{r Libraries}
# General packages
library(sjlabelled)
library(haven)
library(dplyr)
library(tidyverse)
library(splitstackshape)
library(progress)
library(car)

# Visualisation
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(naniar)

# Processing, imputation and more
library(keras)
library(cluster)
library(tensorflow)
library(mice)
library(ggmice)
library(foreach)
library(parallel)
library(doParallel)

# Algorithmic models
library(caret)
library(randomForest)
library(e1071)
library(xgboost)
library(nnet)
library(neuralnet)
library(glmnet)
library(glmnetUtils)
library(ranger)
library(adabag)

# Knitting
library(kableExtra)
library(ggdendro)
library(dendextend)
library(flextable)
```

```{r Loading in data}
# Loading in the results
all_results_df <- read_csv("data/all_results.csv")

# Round the table for comprehensibility
all_results_df[, sapply(all_results_df, is.numeric)] <- lapply(all_results_df[, sapply(all_results_df, is.numeric)], function(x)
  round(x, 3))
```

```{r Cleaning 1}
raw_df <- read_dta('data/scottish.dta')

cleaned_df <- raw_df %>%                             
  select(where(~!any(. == -1, na.rm = TRUE))) %>%  # First, remove all columns that have routing questions
  mutate_all(~ifelse(. %in% c(-9,-99), NA, .)) %>% # Then, all true missing values were coded as -9 | -99
  select(where(~ mean(is.na(.)) < 0.8))            # Finally, remove some columns with more than 80% 
                                                   # missingness; explained below
```

```{r Function - correlation}
calculate_high_correlations <- function(data, threshold) {
  # Calculate correlation matrix
  cor_matrix <- cor(data, use = 'pairwise.complete.obs')
  
  # Extract correlations greater than the threshold
  high_cor <- which(abs(cor_matrix) > threshold & cor_matrix != 1, arr.ind = TRUE)
  
  # Remove duplicate correlations
  high_cor <- high_cor[high_cor[,1] < high_cor[,2],]
  
  # Store column names in col1 and col2
  col1 <- names(data)[high_cor[,1]]
  col2 <- names(data)[high_cor[,2]]
  
  # Print results
  if (length(col2) > 0) {
    cat(paste0("The correlations greater than ", threshold, " are:\n"))
    for (i in 1:nrow(high_cor)) {
      var1 <- col1[i]
      var2 <- col2[i]
      ind1 <- high_cor[i,1]
      ind2 <- high_cor[i,2]
      cat(paste0(var1, " (", ind1, ") and ", var2, " (",ind2, "): ", cor_matrix[ind1, high_cor[i,2]], "\n"))
    }
  } else {
    cat("There are no correlations greater than ", threshold, ".\n")
  }
  
  # Find col1 columns not present in col2
  left_side <- setdiff(col1, col2)
  
  # Return the character vector
  return(unique(left_side))
}
```

```{r Cleaning 2}
# Calculating the pairs with correlation higher than 0.7
cor_result <- calculate_high_correlations(cleaned_df, 0.7)

# Removing the first half of those pairs
cleaned_df <- cleaned_df[, !(colnames(cleaned_df) %in% cor_result)]
```

```{r Cleaning 3}
# Get column names starting with '__', as they are redundant features about name brands
redu1 <- colnames(cleaned_df)[grep("^__", colnames(cleaned_df))]

# Then
redu2 <- colnames(cleaned_df)[grep("^(ma|pa)know[1-5]x?$", colnames(cleaned_df))]

# Then 
redu3 <- colnames(cleaned_df)[grep("^dgof", colnames(cleaned_df))]

# Combine the column names with other redundant features
redu_vector <- c("Refid", "mumscore", "dadscore", "smcost_bands","wwscore", "final_wt",
                 "healthboard","numbrand", "locauth", "museum", "theatre", "SDQscore", 
                 "smokstat", "smokstat2", "subuse", "cgstat", redu1, redu2, redu3)

cleaned_df <- cleaned_df[, !(colnames(cleaned_df) %in% redu_vector)]
```

```{r Variable selection}
# Calculate the percent missing for each column
missing_percent <- colMeans(is.na(cleaned_df))

# Select the columns with more than 10% missingness
selected_cols <- names(missing_percent[missing_percent > 0.10])

# Then determine which variables are eligible
possible_df <- cleaned_df %>% 
  select(all_of(selected_cols)) 
```

```{r Function - dummies}
dummy_maker <- function(data, variable_list) {
  dummy_names <- c()
  
  for (variable in variable_list) {
    dummy_variable <- paste0(variable, "_D")
    data[dummy_variable] <- ifelse(is.na(data[[variable]]), 0, 1)
    dummy_names <- c(dummy_names, dummy_variable) 
  }
  result <- list(data = data, dummy_names = dummy_names)
  return(result)
}
```

```{r Form dummies}
# Make dummies for each possible variable
dummy_result <- dummy_maker(possible_df, colnames(possible_df))
dummy_df <- dummy_result$data

# Remove all non-dummy variables
dummy_df <- dummy_df[, -grep("_D$", names(dummy_df), invert = TRUE)]

# Now, form distance matrix from the correlation matrix of the dummy_df
dist_m <- as.dist(1 - cor(dummy_df))
```

```{r Cluster analysis}
# Perform hierarchical clustering
hc <- hclust(dist_m, method = "average")

# Remove the _DUMMY suffix for interpretability
hc$labels <- sub("_D$", "", hc$labels)

# Cut the clusters with threshold 0.2, meaning clusters can maximally be 80% correlated
clusters <- cutree(hc, h = 0.2)

# Store the target names
dependent_names <- c("welloff", "helptch", "alrisk", "schdrg", "likeskl", "Truant1", "somatic", "dadscore2", 
                  "nothin2", "withfrev2")
```

```{r Function - extract_names}
extract_names <- function(data, cluster_numbers) {
  extracted_names <- list()
  
  for (cluster_number in cluster_numbers) {
    cluster_cols <- names(data[data == cluster_number])
    cleaned_cols <- gsub("_D$", "", cluster_cols)
    extracted_names[[as.character(cluster_number)]] <- cleaned_cols
  }
  
  return(extracted_names)
}

# Define the cluster numbers you want to extract
cluster_numbers <- c(2, 3, 4, 5, 6, 7, 8, 10, 11, 12)

# Extract the names from the selected clusters
extracted_clusters <- extract_names(clusters, cluster_numbers)
```

<!-- Introduction -->

\clearpage

# \LARGE Introduction

Quantitative research within the social sciences regularly involves survey designs, where numerous questions are asked to respondents so data can be generated and analysed. While the goal generally is to have the data be as complete as possible, collected data will usually contain a fair amount of missing responses. Typically, these come in two flavours, with 'unit non-response' referring to the absence of an entire record of a respondent, and 'item non-response' indicating one or multiple items of a respondent are missing. Both present the researcher with their own unique problems, given the researchers had not intended for them to happen. In the case of unit non-response, the sample might not reflect the true population, leading to biased results among other harmful effects (Särndal & Lundström, 2005). A possible solution is weighting of the respondents that the sample did capture, as to construct a more accurate representation of the population. The other kind, item non-response, could be perceived as less abstract, as not entire records of data are missing, but only certain values of records. This essentially forces the researcher(s) to make a decision on how to treat these values, as computationally, almost no analyses can be performed. Among the social sciences, the item non-response problem deserves attention and proper treatment, but is often overlooked or neglected in reality, with crude methods like listwise deletion and mean imputation still being widely popular (Bell, Kromrey & Ferron, 2009; Savage et al., 2021). Both methods can introduce bias into the results, which could lead to inaccurate conclusions.

Fortunately, more sophisticated methods like multiple imputation by chained equations (MICE) and full information maximum likelihood (FIML) are receiving more recognition. In short, the FIML method estimates a likelihood function for each individual based on the variables that are present, thus using all data available. The MICE method works differently by using a series of regression models (where the variable with missing values is taken as the dependent variable, and the independent variables are the remaining variables) to make predictions for the missing values. A random component is added to the predictions to emulate a level of uncertainty. Normally, MICE will perform multiple cycles, constantly updating the regression models with the imputed values of the earlier cycle (Van Buuren & Groothuis-Oudshoorn, 2011). Except for a few specific situations, research has shown that MICE and FIML consistently outperform the aforementioned listwise deletion and mean imputation (Wulff & Jeppesen, 2017; Witte, Foraita & Didelez, 2022), and will produce unbiased imputations if the missing data was generated by specific mechanisms (Wulff & Jeppesen, 2017). Modern literature divides these missing data mechanisms in three distinct categories:  Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR), also called Not Data Dependent, Seen Data Dependent and Unseen Data Dependent, respectively (Van Buuren & Groothuis-Oudshoorn, 2011). In summary, when data is MCAR, the missingness is unrelated to the observed as well as the unobserved data. An example of this would be if data is not recorded because of an accidental technical diﬀiculty during an online survey. Data is MAR when the probability of missingness is related to the observed data. Imagine a survey is held among a population to monitor anxiety, and sex of the participant is recorded as well. Assume for the sake of the example that males have a harder time answering the questions regarding anxiety, and tend to skip them. Data would then (rather counterintuitively) be called Missing At Random, as the probability of missing is dependent of sex of the respondent. Finally, MNAR implies the reason data is missing because it is related to unobserved data. In the aforementioned example about the anxiety survey, data would be MNAR if sex of the respondent had not been observed, meaning the probability of missingness on questions concerning anxiety would now be dependent on unobserved data.

Accordingly, the sophisticated methods mentioned earlier make use of this observed data to compute imputations for the missing values. Therefore, they can only produce unbiased results if data are either MCAR or MAR. Naturally, these methods will fail to account for the non-response bias in their results if the data are MNAR, as the unobserved variables (the underlying cause of the missingness) are not present in the data. So although they are shown to perform better than listwise deletion and mean imputation (and other crude methods), researchers might not feel comfortable implementing these methods as they will have to determine what the missing data mechanism is. Especially in the social sciences, of which its students have been shown to be the least statistically literate among peers (Pan & Tang, 2004; Berndt et al., 2021), a certain hesitancy might be structurally present to consider these modern instruments. Determining what the missing data mechanism is requires the researcher to anticipate what variables might have missing values, and what other variables might account for their missingness. Data on those variables should then also be collected. Identifying which variables can account for this missingness, however, presents its own set of challenges. While the general topic of non-response is widely explored, validated information about specific constructs (like what may cause respondents to refrain from answering questions about depression) is missing, leaving the researcher to resort to speculation. To remove the need for this scientifically ambiguous process, this thesis project will aim to enrich a common knowledge base about these specific variables by predicting missingness with the assistance of algorithmic modelling. This knowledge base could lead to researchers (in particular in the social sciences) experiencing less of a barrier to implement methods like MICE and FIML. Subsequently, more accurate data and interpretations could be produced.

As the necessity and relevancy of a trustworthy scientific process is at the core of this project, principles of Open Science were applied. These principles aim to attain transparency, collaboration and accessibility by ensuring data is publicly available, research is able to be reproduced, ethical considerations and various other practices (Vicente-Saez & Martinez-Fuentes, 2018). Additionally, in consideration of possible harmful biases that may occur when the researchers degrees of freedom are not accounted for, this project utilises the novel concept of multiverse analysis (Steegen, Tuerlinckx, Gelman & Vanpaemel, 2016). The researcher degrees of freedom refer to the inherent nonuniformity of scientific experiments, as researchers can choose from a variety of different methods and approaches to conduct the data collection and analysis processes. Data dredging is one of those instances, for example, where the researcher degrees of freedom are not taken into account; Exhaustive analysis of the data and deliberately only reporting a particular set of results may create a skewed representation of the data (Smith & Ebrahim, 2002). Aiming to counteract these biases, multiverse analysis makes use of multiple methods and assesses whether results congrue among the methods used, thus increasing reliability and transparency. Regarding this project, multiverse analysis was implemented by using various (appropiate) machine learning algorithms, along with different parametrizations. Results of all of the algorithmic models were reported regardless of quality.

The data analysed in this project is from a survey concerning mental health, and alcohol and drug use among Scottish adolescents. Multiple studies have shown the majority of adults with a substance use disorder first comes in contact with alcohol and drugs as adolescents (Gutierrez & Sher, 2015), so research on these topics is crucial at this age. Moreover, adolescents have been shown to be increasingly vulnerable to mental health problems in general (Kieling et al, 2011). Accordingly, it is of significant importance to make valid inferences, which in terms of quantitative research, requires the data to be accurate. Exploring what variables might be associated with non-response on these topics can help future research with including them, which in combination with missing data techniques decreases the bias the data might have. Hence, the research question of this thesis project is: “What variables are able to consistently predict item non-response among Scottish adolescents through multiverse modelling?”

A short description of the data is given first. This is followed by the methodology, consisting of a detailed description of the steps taken in processing the data. All of the selected algorithmic models are also elaborated on in this section. Subsequently, the results are listed, proceeded by the discussion and limitations. Ethical considerations and the conclusion complete the project. All files (except for the data, which is discussed later) and documentation are available at https://github.com/LDvLeeuwen/Thesis_LDvL.git. Note that the nature of the project was exploratory, and that reproduction is highly encouraged.


<!-- Data -->


# \LARGE Data

The survey consisted of 89 questions and was conducted in 2018 by Ipsos MORI Scotland. The target population was adolescents ages 12 through 18, living in Scotland. Aside from alcohol consumption, mental health, and drug use, the survey discusses topics like parental supervision, leisure activities and friendships. To illustrate, *“Have you ever had a proper alcoholic drink - a whole drink, not just a sip?”*, *“Have you ever been offered powders or pills that are sold as legal highs?”* and *“How many close friends would you say you have?”* were among the questions included in the survey. A “Strengths and Difficulties” questionnaire was included at the end, and measured a variety of mental health constructs with statements as *“I worry a lot”* and *“I have many fears, I am easily scared”*. 

The final data set provided by Ipsos consists of 635 columns, with a sample of 23.365 respondents. The reason for the large discrepancy between the number of questions and number of columns is because a lot of columns hold the same information, but are coded or grouped differently. Moreover, the presence of questions that consist of multiple statements regarding the same topic causes some questions to translate to twenty or more columns in the data. The data is publicly available (as long as the intention with the data is non-commercial; see appendix A). Skip patterns (also named ‘routing’) were also present in the data. Skip patterns intend to increase eﬀiciency of a survey by only asking certain questions to a subset of the sample based on answers on earlier questions. The missing responses (if the question was indeed not asked to a respondent as a result of a skip pattern) on these items were coded as ‘-1’ in the data. Missing values as a result of the respondent not answering the question were coded as ‘-9’. Naturally, only the latter are of interest in the scope of this project, as the researchers did not intend for these values to be unrecorded.


<!-- Methodology -->


# \LARGE Methodology

In this section, an overview of the steps taken to process the data is provided, along with information about the algorithmic models and why they were deemed appropiate for the analysis. All of the processing of the data and algorithmic modelling was done in R 4.2.1. Machine learning was implemented with the `caret` package (Kuhn et al., 2020). To ensure comprehensibility, trivial steps concerning the processing were not described, but are explained in the source code.

## \Large Data preparation

The data was prepared by removing irrelevant data, selecting which variables could be taken as dependent variables based on a cluster analysis, and finally, imputations with MICE.

### \large Data cleaning

First off, data containing skip pattern questions were deemed irrelevant, and were removed accordingly. While this may appear like unnecessary data reduction, the provided codebook and example survey were examined thoroughly to support this decision. The survey was designed to include every kind of drug, for instance, which were only asked to adolescents who said that they had ever taken drugs. This results in variables that are very highly correlated, and essentially carry the same information. Another example is question 12: *“How many cigarettes did you smoke on each day in the last 7 days, ending yesterday?”*, which end up as 7 columns that all highly correlate with each other, and are therefore redundant for the analyses. Secondly, a correlation analysis was performed to discern whether variables held similar information. Correlations of each pair of variables were calculated with pairwise comparison, and as to not remove essential data, only the first variable of the pair was removed if correlations rose above a coeﬀicient of 0.7. Lastly, a couple of variables were deleted as a result of qualitative assessment. Administrative variables like the respondent ID, variables that convey information about the same construct but were coded differently, and trivial variables about cigarette brands all fall into this category.

### \large Dependent variable selection 

Of the remaining 192 variables, the missingness of 74 variables were considered a possible contender as a dependent variable, as they contained more than 10% missing values. Next, a cluster analysis was performed to determine whether variables followed a similar missingness pattern. In other words, if the missing values in two or more variables occur in a corresponding order, they should not be analysed separately, since they virtually carry the same information regarding missingness. 

To realise this, dummies were constructed based on the missingness of these variables. Next, a correlation matrix was calculated, essentially measuring the similarity of missingness of each pair of dummies. By converting the similarity matrix to a distance matrix, hierarchical clustering (with average linking) could be performed. The result is shown by figure 1:

```{r Dendrogram, include = TRUE, echo = FALSE, fig.width = 12, fig.height = 15, fig.align = "center", fig.cap= "Cluster dendrogram", fig.pos = "H"}
par(cex=1, mar=c(3, 2, 2, 5))
dend <- as.dendrogram(hc)

dend <- color_branches(dend, h = 0.2, col = c("dodgerblue2", "red", "mediumpurple1","lightgreen", "darkblue", "red3",
                                              "mediumpurple4", "green4", "dodgerblue2", "red", "mediumpurple1", 
                                              "lightgreen")) %>% 
  set("branches_lwd", 3.3) %>% 
  plot(horiz=TRUE, axes = TRUE)


abline(v = 0.2, lwd = 1, lty = 2, col = "black")

mtext("Distance Score", side = 1, line = 2.2, at = 0.4, cex = 1.3)
```

A cutoff of 0.2 was applied for the distance score, leading to a total of twelve clusters. Two of those clusters were excluded from the analysis. The resulting ten clusters are listed below in table 1, along with the survey questions corresponding to the variables. 

```{r Preparation table 1}
subjects <- c("How well off the adolescent feels about the family they live with", "Where/who the adolescent would go to if they
              felt that they needed help because you were using alcohol/drugs, who/where would you go to?", "How much information
              school has provided the adolescent about alcohol/drugs, according to the adolescent", "How much advice/support
              school has provided the adolescent about alcohol/drugs, according to the adolescent", "How much the adolescent is
              enjoying school", "In the past year, how much the adolescent has skipped school", "Strengths & Difficulties
              questionnaire, various mental health constructs", "How much the parental figures know about them and their activies,
              according to the adolescent", "Whether the adolescent does nothing in their free time", "How much evenings the
              adolescent spends with friends in a typical week")
```

```{r Table 1, include = TRUE, echo = FALSE, fig.align = "center", results = "asis", fig.pos = "H"}
desired_order <- 1:length(extracted_clusters)

# Create a new list to store the reordered clusters
reordered_clusters <- vector("list", length(desired_order))

# Iterate over the clusters
for (i in 1:length(extracted_clusters)) {
  cluster_index <- match(i, desired_order)  
  reordered_clusters[[cluster_index]] <- extracted_clusters[[i]]  
}

# Update the extracted_clusters object with the reordered clusters
extracted_clusters <- reordered_clusters

table1_clusters <- data.frame(Cluster_number = 1:10,
                              Cluster_subject = subjects,
                              Variables = character(length(extracted_clusters)),
                              stringsAsFactors = FALSE)

for (i in 1:length(extracted_clusters)) {

  onecluster <- extracted_clusters[[i]] 
  # Assign the variable names to the corresponding row in the dataframe
  table1_clusters$Variables[i] <- paste(onecluster, collapse = ", ")
}

colnames(table1_clusters)[which(names(table1_clusters) == "Cluster_number")] <- "Cluster number"
colnames(table1_clusters)[which(names(table1_clusters) == "Cluster_subject")] <- "Cluster subject"

#table1_clusters$Variables <- sapply(table1_clusters$Variables, function(x) linebreak(strsplit(x, " ")))

# kbl(table1_clusters, caption = "Cluster subjects", booktabs = TRUE) %>% 
#   kable_styling(latex_options = c("striped", "HOLD_position", "scale_down")) %>% 
#   column_spec(1, width = "2cm", latex_valign = "m") %>%
#   column_spec(2, width = "5cm", latex_valign = "m") %>%
#   column_spec(3, width = "7cm", latex_valign = "m") 

kbl(table1_clusters, format = "latex", longtable = TRUE, caption = "Cluster subjects", booktabs = TRUE) %>% 
  kable_styling(latex_options = c("striped", "HOLD_position")) %>% 
  column_spec(1, width = "2cm", latex_valign = "m") %>%
  column_spec(2, width = "5cm", latex_valign = "m") %>%
  column_spec(3, width = "7cm", latex_valign = "m") 
```


Both of the excluded clusters only consisted of one dummy: One is the dummy indicating missingness in the variable `wwbands`. This variable holds a score that consists of all of the questions of the “Strengths and Diﬀiculties” questionnaire compiled together, and is thus not based on response. The second cluster is the dummy indicating missingness in `scmost`. This variable holds information about how much adolescents thought a packet of cigarettes would cost, and was deemed irrelevant in the scope of the project. 

A lot of the clusters consist of variables that were formed by the same question in the survey, which explains the similarity in missingness pattern. An example of this is cluster 3, where most of the variables represent an option to the question *"In school, how much have you learned about the following?"*, with the options being *"The risks to your health from cigarettes"*, *"The risks to your health from alcohol"*, and so forth. 

One variable of the ten remaining clusters was selected arbitrarily for analysis, and are listed below in table 2, along with the amount of missingness. In preparation of the data analysis, a different data frame was formed for each cluster. These consisted of all of the preprocessed 192 variables minus the variables inside the cluster in question. Additionally, the binary dummy variable indicating the presence of missing values for the selected variable of that cluster is included in the respective data frame.

```{r Table 2, include = TRUE, echo = FALSE, fig.align = "center", results = "asis", fig.pos = "H"}
table2_missingness <- data.frame(Cluster = 1:10,
                                 Variable = dependent_names,
                                  Missingness_Percentage = numeric(length(dependent_names)), 
                                  stringsAsFactors = FALSE)

for (i in seq_along(dependent_names)) {
  variable_name <- dependent_names[i]
  missing_percent <- round(mean(is.na(cleaned_df[[variable_name]])) * 100,2)
  table2_missingness[i, "Missingness_Percentage"] <- missing_percent
}

colnames(table2_missingness)[which(names(table2_missingness) == "Missingness_Percentage")] <- "Missingness (in %)"

kbl(table2_missingness, caption = "Missingness in the dependent variables", booktabs = TRUE) %>% 
  kable_styling(latex_options = "HOLD_position")
```

### \large Imputation with MICE

MICE was used (with package `mice`) to impute the missing values in the data. Having extensively described the potential harm of applying MICE when the data might not be MCAR/MAR, this may appear tricky. Prohibiting the use of MICE, however, creates a circular argument, as the goal of this project is to uncover the underlying response mechanisms so future research can make sure potential missing data would follow a MCAR/MAR pattern. For this reason, MICE was still opted for, with additional assessment of the quality of imputations (which is mentioned later). 

Since there are 10 distinct data sets for each of the dependent variables, imputations were computed for each of those frames. To make the process more efficient, the `quickpred` function was used, a method described by Stef van Buuren (2018). This method makes a selection of predictors, instead of taking every possible variable as a predictor, which is the default. The result is a predictor matrix for the data frame, stating which variables are allowed to predict what variables through a binary system. The selection is done by calculating two types of correlations: The first correlation uses the values of the target and the predictor directly, whereas the second correlation uses the response indicator of the target and the values of the predictor. If the largest of these correlations exceeds the argument `mincor` (of which a value of 0.2 was selected), the predictor will be added to the matrix. Additionally, the procedure eliminates predictors whose proportion of usable cases fails to meet the minimum specified by the argument `minpuc`, of which a value of 0.2 was implemented. Consequently, the resulting matrix is included in the imputation process through the `predictorMatrix` argument in the `mice` function. This drastically speeds up the process, as generally the entire data frame, but roughly 25 variables are used as predictor (for each variable with missing values).

Furthermore, a `m` of 1 and a `maxit` of 3 were implemented. The `m` parameter denotes the amount of imputations MICE will make for each missing value. As variables are not selected on p-values or confidence intervals in this project (but rather if they harmonize among different algorithmic models), standard errors are less important. Therefore, a single imputation is appropiate. The `maxit` argument indicates how much iterations the MICE algorithm should run. In order to let the imputations converge, but to be mindful of the computational cost, a `maxit` of 3 was selected. 

Lastly, the imputation technique for each variable. MICE requires a specification of a univariate method for each variable to be imputed. Not every method is appropiate for each type of data: For instance, logistic regression should only be used for binary variables. If these are not specified, MICE uses predictive mean matching (`pmm`) for numeric variables, logistic regression (`logreg`) for binary variables, and polytomous logistic regression (`polyreg`) for multiclass variables as a default. These default options were deemed appropiate for the data, and since all variables were converted to the correct data type beforehand (numeric as numeric, categories as factors), no specifications were given for this argument. Afterwards, this was checked with the `method` element of the imputation objects. The `loggedEvents` element (which keeps track of all shortcomings like multicollinearity) was also inspected for each set of imputations, but none were reported. 

## \Large Analyses

The goal of this project can essentially be translated to a binary classification problem, as the algorithms are trained to predict missingness in a variable by taking a respective dummy (where 0 is coded as ‘missing’ and 1 as ‘present’) as the target. As table 2 visualises clearly, all of the ten target variables roughly have the same missing rate at 12%. The two classes are decidedly very imbalanced, which might pose a problem; For example, one could predict every case as ‘present’, and would be right in 88% of those cases. Balanced accuracy and the Matthews correlation coefficient were therefore used as the main evaluation metrics, as they both provide a more accurate assessment of model performance on imbalanced data. Balanced accuracy does this by taking both the sensitivity (the proportion of actual positive instances that are correctly identified as positive) and the specificity (the proportion of actual negative instances that are correctly identified as negative) into account. On the other hand, the Matthews correlation coefficient is calculated with all four possible outcomes of binary classification (true positives, true negatives, false positives, and false negatives), ensuring that all measures of performance are taken into consideration. Formulas are given below. 

\[
\text{Balanced Accuracy} = \frac{\text{Sensitivity} + \text{Specificity}}{2}
\]

\[
MCC = \frac{(TP \times TN) - (FP \times FN)}{\sqrt{(TP + FP) \times (TP + FN) \times (TN + FP) \times (TN + FN)}}
\]

Moreover, algorithmic models whose performance has been shown to be gravely affected by class imbalance, like Logistic Regression or Linear Discriminant Analysis, were therefore not included (Brown & Mues, 2012). Algorithms who have been shown to struggle with large amounts of data (Naive Bayes, for instance) were also rejected. Furthermore, as variable evaluation is central to this project, algorithms like Support Vector Machines (which uses high-dimensional spaces, making interpretation of individual variables harder), were also not included. Utimately, the Random Forest, Neural Networks, XGBoost Regression and Adaptive Boosting algorithms were used. In the scope of the project, the models have to perform well on unseen data, so the importance of the variables belonging to those models can be generalised beyond the data used in this project. In order to evaluate how well the models perform on unseen data, the data was split in a training and a test set (with a margin of 70% for the training set). Tuning of the models was thus only applied on the training set. To reduce the chance of overfitting, and in the spirit of multiverse analysis, different training and test sets were formed for each algorithm. Overfitting occurs when a model becomes too complex and starts to "memorize" the training data rather than learning the underlying patterns. The splitting of the training and test sets was performed before each algorithm was implemented in the code, and by changing the seed (with `set.seed`) each time the data was split, the training and test sets were different for each algorithm. Three distinct tuning methods were utilized, of which a general description is listed below. Afterwards, a detailed explanation about the implementations of each algorithmic model is given.

### \large Tuning methods

#### \normalsize Hyperparameter grid search 
\
\
Hyperparameters refer to the variables that define the behaviour and performance of a machine learning algorithm. These differ with each algorithm, and tuning them allow for better model performance. For this project, a hyperparameter grid search was performed for each algorithm. This is implemented by manually setting a grid with possible values for different (with the `caret::train` function), thus forming a grid. Each possible combination of hyperparameters is then tested on the training set, of which one combination will facilitate a best-performing model. The grid search was opted for as hyperparameters as the grid can be set manually, and is computationally efficient in comparison with other hyperparameter tuning methods like genetic algorithms. Since the hyperparameters differ with each algorithm, they are elaborated on in their respective sections. 

#### \normalsize Class weights 
\
\
Class weights were chosen as the first method to account for the imbalanced data. They assign higher weights to the minority class, and lower weights to the majority class, aiming to improve performance on the minority class. Inverse class weights were calculated for each dependent variables, of which a formula is listed below. They were not utilised for the Adaptive Boosting algorithm, since weighting of the classes is  inherent to the algorithm itself. While tuning of the class weights is possible and offers a more refined way to calculate these weights, this is very computationally heavy, and could not be realised because of time constraints.

\[
weights = \frac{1}{N_{class}}
\]

#### \normalsize Threshold tuning
\
\
Threshold tuning is the second technique used in to account for imbalanced data. In classification, an algorithmic model will assign probabilities to each class, using a threshold to ultimately predict a class label. While the standard is a threshold of 0.5, this threshold can be optimised with tuning. In the scope of the project, thresholds ranging from 0.05 to 0.5 (with intervals of 0.05) were used on the ‘missing’ class. For instance, if the model makes a prediction of 0.32 for the value to be ‘missing’, and the current threshold tested is 0.2, the model will predict the value to be ‘missing’. Naturally, if a standard threshold had been applied, this prediction would have been ‘present’. Threshold tuning was performed only on the training sets, and the optimal threshold was consequently used for the predictions on the unseen test set. 

The hyperparameter grid search and class weights models were both validated by k-fold cross-validation. This process cuts the data in an given amount of sections, training a model on all sections except one, and testing in it on the section that was left out. Cross-validation was opted for as it prevents the given methods from overfitting. All of the tuning methods were evaluated on balanced accuracy (for the grid search and class weights models, this was implemented with the `metric` argument in `trainControl`). The stratified variant was applied to prevent the folds from having too few data points of the ‘missing’ class, and the data was divided in 5 folds for each instance of cross-validation.

Generally, three or two versions (for each algorithm and for each dependent variable) were tested at the same time. Threshold tuning was then applied on the best-performing model (which was evaluated with the balanced accuracy and MCC). If another threshold than 0.5 was calculated, new predictions were made. While this does not inherently change the structure of the model (or the importance of the variables), it does inform whether a model would perform better with a different threshold.

### \large Algorithmic models

#### \normalsize Random Forest
\
\
The Random Forest is an ensemble learning algorithm that combines multiple decision trees to create a powerful predictive model. Each decision tree in the random forest is trained on a random subset of the training data (on rows as well of columns of the data), and the final prediction is obtained by aggregating the predictions of all individual trees. Regarding classification, this aggregation is done by taking the majority vote. Moreover, since each tree is trained on a different subset of the data, the algorithm is less prone to overfitting. The Random Forest has been shown to be well-suited for classification tasks as well as having the ability to handle large data sets with high dimensionality (Speiser, Miller, Tooze & Ip, 2019), and for these reasons, the algorithm was selected for the analyses.

The `randomForest` package was used for the baseline model. Then, the `caret` package with module `ranger` was used to tune hyperparameters with a grid search. The `mtry`, `splitrule` and `min.node.size` hyperparameters were tuned. `mtry` determines the number of variables randomly selected as candidates at each split point in a decision tree. Generally, larger values of mtry can improve the model's ability to capture complex relationships but may consequently increase the risk of overfitting. In the grid search, `mtry` was tested for values 1, 5 and 15. The `splitrule` hyperparameter determines the criterion used for making splits in a decision tree. The `caret` package supports two commonly used split rules, "Gini" and "Extra trees". The "Gini" split rule, also known as Gini impurity, measures the impurity or homogeneity of a node based on the class distribution of the training samples. It aims to minimize the probability of misclassifying a randomly chosen sample. The "Extra trees" split rule, short for extremely randomized trees, is a variant of random forests that introduces additional randomness to the split point selection process. This results in faster training times but may also increase the tree's variability. Both methods were included in the grid search. Lastly, the `min.node.size` hyperparameter specifies the minimum number of samples required to create a terminal node (leaf) in a decision tree. When growing a tree, if the number of samples in a node falls below `min.node.size`, further splitting is not allowed, and the node becomes a leaf node. Increasing results in smaller trees with more generalization and smoother decision boundaries. Conversely, decreasing allows trees to capture more specific patterns and can lead to more complex models. For `min.node.size`, values 1, 5 and 10 were tested. A standard ntree of 500 was used and was left unchanged, as the `ranger` module does not support tuning of this hyperparameter (Kuhn et al., 2020).

Three versions of the Random Forest algorithm were implemented: A base version, a parameter tuned version and one with class weights and parameter tuning. Predictions on the test set were made at the same time for all three versions. Threshold tuning on the training sets was applied on all of the parameter tuned versions, as those performed best for each independent variable. Subsequently, new predictions were made with the optimal thresholds.

#### \normalsize Neural Networks
\
\
Neural Networks are a class of deep learning models inspired by the structure and functioning of the human brain, hence the name. They consist of interconnected nodes, called neurons, which organized in layers. Each neuron applies a mathematical transformation to its inputs and passes the result to the next layer until the final output is generated. 

In order to run the analyses, categorical variables were one-hot encoded. Implementing the base version of the Neural Networks algorithm was done with the `nnet` package. The `caret` package (module `nnet`) was used to tune the `size` and `decay` parameters with a grid search.  `size` determines the number of neurons in the neural network. Increasing the size value adds more neurons to the network, which can increase its capacity to learn complex patterns and relationships in the data, with the added risk of overfitting. Decreasing therefore leads to reduced performance, but less chance of overfitting. For `size`, values 5, 10, 20 and 50 were tested. The `decay` hyperparameter controls the weight decay applied. Weight decay is a regularization technique that introduces a penalty term to the loss function during training to prevent overfitting. Increasing this penalty thus leads to a bigger chance of underfitting (the opposite of overfitting, where the model does not capture the complexities of the data). Values 0 (essentially no penalty), 0.001, 0.01 and 0.1 were included in the grid for `decay`.

A base model, a parameter tuned version and a version with inverse class weights and parameter tuning were implemented for the Neural Networks algorithm: Predictions on the test set were made at the same time for all three versions. Threshold tuning on the training sets was applied on the models that had inverse class weights and parameter tuning. For three of those, another optimal threshold than the standard 0.5 was calculated. These were models for clusters 1, 5 and 7, and for all three, the optimal threshold on the training data was 0.45. Naturally, new predictions with these threshold were made on the test set. 

#### \normalsize XGBoost Regression
\
\
The XGBoost Regression algorithm (short for Extreme Gradient Boosting) constructs an ensemble of weak prediction models in a sequential manner. New models are trained to correct the errors made by the previous ones, gradually improving the final prediction accuracy. It has been shown to excel in eﬀiciency and performance (Ramraj, Uzir, Sunil & Banerjee, 2016), and was therefore selected for the analyses.

In order to run the analyses, categorical variables were one-hot encoded. The `xgboost` package was used to perform the analyses for the base models. Again, the grid search was realised with `caret` package (module `xgbTree`). The XGBoost algorithm has a lot of hyperparameters: the `nrounds`, `max_depth`, `eta`, `gamma`, `colsample_bytree`, `min_child_weight` and `subsample` were all included in the grid. The `nrounds` hyperparameter determines the number of boosting rounds or iterations performed during the training process. Each boosting round adds a new decision tree to the ensemble, gradually improving the model's predictive performance. For `nrounds`, values 100, 200 and 300 were opted for. `max_depth` controls the maximum depth of an individual decision tree within the ensemble, and values 3, 6 and 9 were included. Generally, for both `nrounds` and `max_depth`, as their values increase, the chance of overfitting also increases, while performance also improves.

The `eta` hyperparameter, also known as the learning rate, determines the step size at each boosting iteration, controlling the contribution of each tree to the overall ensemble. Values 0.1, 0.3 and 0.5 were opted for. `gamma` controls the minimum loss reduction required to make a further partition on a leaf node of the tree, while `min_child_weight` defines the minimum sum of instance weights required in a leaf node. For these hyperparameters, values 0, 0.1, 0.2 and 1, 3 and 5 were used respectively. Contradicting the first two hyperparameters, as values for `eta`, `gamma` and `min_child_weight` increase, the chance of overfitting is reduced. This will negatively affect the performance, though.

Finally, `colsample_bytree` determines the fraction of variables to be randomly sampled for each tree, whereas the `subsample` hyperparameter controls the fraction of training samples to be used for each boosting iteration. Values 0.6, 0.8 and 1 were included for both of them. As a general rule, setting their values beneath 1 leads to a decrease in overfitting, and improves generalisation.

Unfortunately, a version with inverse class weights could not be implemented due to an unresolved error. Two versions of the XGBoost algorithm were thus tested: A base version and a parameter tuned version. For both versions, predictions on the test set were made at the same time. Threshold tuning on the training sets was applied on all of the parameter tuned versions, as those performed best for each independent variable. For all 10 models, a better optimal threshold than 0.5 was calculated, and new predictions were made using these improved thresholds. 

#### \normalsize Adaptive Boosting
\
\
Adaptive Boosting, also called AdaBoost, combines multiple weak classifiers to construct a strong classifier. Assigning weights to each training sample in subsequent iterations, the misclassified samples are emphasised to improve classification accuracy. Research has demonstrated its effectiveness (Margineantu &	Dietterich, 1997; Feng et al., 2020), and as it is designed to handle class imbalance, the algorithm was selected for the analyses.

The baseline models for the Adaptive Boosting algorithm were implemented with package adabag. Tuning of hyperparameters `mfinal`, `maxdepth` and `coeflearn` was done along a grid with the `caret` package (module `AdaBoost.M1`). `mfinal` determines the maximum number of weak classifiers to be combined in the final ensemble, whereas the `maxdepth` hyperparameter specifies the maximum depth or complexity of the weak classifiers used in the ensemble. Increasing both of them can result in a more complex model. However, this also increases the risk of overfitting, especially if the dataset is small. For `mfinal`, values 50, 100 and 150 were included, whereas the values of `maxdepth` consisted of 2, 3 and 4.

Lastly, the `coeflearn` hyperparameter specifies which learning method is used in AdaBoost. `caret` supports three methods: Freund’s, Breiman’s, and Zhu’s. Freund’s method increases the weights of misclassified samples in each iteration, effectively making the subsequent weak classifiers focus more on these difficult samples. The weights are updated using an exponential function. Breiman’s method, on the other hand, modifies the weight update formula to be based on the misclassification rate rather than the exponential function used in Freund's method. The weights of the misclassified samples are increased, but the increase is proportional to the misclassification rate. To conclude, Zhu's method incorporates the concept of cost-sensitive learning. 

Two versions of the Adaptive Boosting algorithm were implemented: A base version and a parameter tuned version. Predictions on the test set were made at the same time for both versions. Threshold tuning on the training sets was applied on all of the parameter tuned versions, as those performed best for each independent variable. Subsequently, new predictions were made with the optimal thresholds.


<!-- Results -->


# \LARGE Results

## \Large Best-performing models

The balanced accuracy metric was chosen to evaluate which version of the model performed best. A best-performing model was selected per target, per algorithm. Results are listed below in table 3, accompanied with their specificity, sensitivity, F1-score, balanced accuracy and the MCC. Results of all versions can be viewed in appendix B. Please note that for legibility, the dummy indicator was removed from the variables in the *Target* column. As mentioned in the Methodology section, the variables themselves were not the target variables, but the missingness of those variables. 

```{r Best results}
# Change column names for legibility
colnames(all_results_df)[which(names(all_results_df) == "Sensitivity")] <- "Sens."
colnames(all_results_df)[which(names(all_results_df) == "Specificity")] <- "Spec."
colnames(all_results_df)[which(names(all_results_df) == "F1_Score")] <- "F1"
colnames(all_results_df)[which(names(all_results_df) == "Balanced_Accuracy")] <- "Bal. Acc"
colnames(all_results_df)[which(names(all_results_df) == "Matthews_Correlation")] <- "MCC"

# Remove the Dummy indicator
all_results_df$Target <- gsub("_D$", "", all_results_df$Target)
```

```{r Preparation table 3}
# Create an empty data frame to store the selected rows
table3_topmodels <- data.frame()

# Get the unique targets and models in the data frame
unique_targets <- unique(all_results_df$Target)
unique_models <- unique(all_results_df$Model)

# Iterate over each unique combination of target and model
for (i in seq_along(unique_targets)) {
  target <- unique_targets[i]
  #cluster <- i

  for (model in unique_models) {
    # Subset the data frame for the current combination of target and model
    subset_df <- all_results_df[all_results_df$Target == target & all_results_df$Model == model, ]

    # Find the row with the highest "Bal. Acc" value in the subset
    max_row <- subset_df[which.max(subset_df$"Bal. Acc"), ]

    # Append the row to the top_model_df dataframe
    table3_topmodels <- rbind(table3_topmodels, max_row)
  }
}

# Add the "Cluster" column to the new dataframe as the first column
table3_topmodels <- cbind(Cl. = rep(1:length(unique_targets), each = length(unique_models)), table3_topmodels)

# Reset row names of the new dataframe
row.names(table3_topmodels) <- NULL
```

```{r Table 3, include = TRUE, echo = FALSE, fig.align = "center", results = "asis"}
kbl(table3_topmodels, format = "latex", longtable = TRUE, caption = "Best model results", booktabs = TRUE) %>% 
  kable_styling(latex_options = c("striped", "repeat_header")) %>%
  column_spec(1, width = "4mm", latex_valign = "m") %>% 
  column_spec(2, width = "14mm", latex_valign = "m") %>%
  column_spec(3, width = "15mm", latex_valign = "m") %>%
  column_spec(4, width = "3cm", latex_valign = "m") %>%
  column_spec(5:9, width = "8mm", latex_valign = "m")
```

## \Large Variable importance

The variable importance for each best-performing model version was calculated with the `varImp` function of the `caret` package. The absolute importance coefficient was recorded as well as the relative importance. The relative importance is scored on a scale from 0 to 100, where 100 indicates the variable is the most important compared to the other variables in the model. The 5 most important variables per dependent variable were listed for comprehensibility. Full results are available in the aforementioned GitHub repository.

\newpage

### \large Random Forest

For the Random Forest models, the variable importance was calculated with the permutation method in `caret`. Results are listed below in tables 4 and 5. 

```{r Table 4 and 5 Preparation}
all_varImp_df <- read_csv("data/all_varImp_10C.csv")

# Round for legibility
all_varImp_df[, sapply(all_varImp_df, is.numeric)] <- lapply(all_varImp_df[, sapply(all_varImp_df, is.numeric)],
                                                                     function(x) round(x, 4))

# Group by Dependent and select the Variables with the highest ImportanceScaled scores
table4_RFvar <- all_varImp_df %>%
  filter(Model == "Random Forest") %>% 
  group_by(Dependent) %>%
  slice_max(ImportanceScaled, n = 5) %>%
  ungroup() %>%
  arrange(factor(Dependent, levels = dependent_names)) %>% 
  select(-Model)

# Change column name for consistency with other tables
colnames(table4_RFvar)[which(names(table4_RFvar) == "Dependent")] <- "Target"

# Add cluster number
table4_RFvar <- cbind(Cl. = rep(1:10, each = 5), table4_RFvar)
```

\begin{minipage}{0.52\textwidth}

```{r RF table 1, include = TRUE, echo = FALSE, results= "asis"}
colnames(table4_RFvar)[which(names(table4_RFvar) == "ImportanceScaled")] <- "Imp. Scaled"
colnames(table4_RFvar)[which(names(table4_RFvar) == "ImportanceRaw")] <- "Imp. Raw"

kbl(table4_RFvar[1:25,], format = "latex", caption = "RF importance", booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  column_spec(3, width = "12mm") %>% 
  column_spec(4:5, width = "1cm")
  
```

\end{minipage}
\begin{minipage}{0.52\textwidth}

```{r RF table 2, include = TRUE, echo = FALSE, results = "asis"}
kbl(table4_RFvar[26:50,], row.names = FALSE, caption = "RF importance (cont.)", format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  column_spec(3, width = "12mm") %>% 
  column_spec(4:5, width = "1cm")
```
\end{minipage}

\newpage

### \large Neural Networks

For the Neural Network models, `caret` calculates variable importance based on a method by Gevrey et al (2003). As categorical variables had to be one-hot encoded to be compatible for the `caret` package, interpreting their importance is tricky, as they cannot simply be added up. Unfortunately, due to time constraints, a more primitive method was applied: Of all the categories, the highest importance was taken, and selected to represent the whole variable. Results are listed below in tables 6 and 7. 

```{r Table 6 and 7 Preparation}
all_varImp_df <- read_csv("data/all_varImp_10C.csv")

# Round for legibility
all_varImp_df[, sapply(all_varImp_df, is.numeric)] <- lapply(all_varImp_df[, sapply(all_varImp_df, is.numeric)],
                                                                     function(x) round(x, 4))

# Group by Dependent and select the Variables with the highest ImportanceScaled scores
table6_NNvar <- all_varImp_df %>%
  filter(Model == "Neural Networks") %>% 
  group_by(Dependent) %>%
  slice_max(ImportanceScaled, n = 5) %>%
  ungroup() %>%
  arrange(factor(Dependent, levels = dependent_names)) %>% 
  select(-Model)

# Change column name for consistency with other tables
colnames(table6_NNvar)[which(names(table6_NNvar) == "Dependent")] <- "Target"

# Add cluster number
table6_NNvar <- cbind(Cl. = rep(1:10, each = 5), table6_NNvar)
```

\begin{minipage}{0.52\textwidth}

```{r NN table 1, include = TRUE, echo = FALSE, results= "asis"}
colnames(table6_NNvar)[which(names(table6_NNvar) == "ImportanceScaled")] <- "Imp. Scaled"
colnames(table6_NNvar)[which(names(table6_NNvar) == "ImportanceRaw")] <- "Imp. Raw"

kbl(table6_NNvar[1:25,], format = "latex", caption = "NN importance", booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  column_spec(3, width = "12mm") %>% 
  column_spec(4:5, width = "1cm")
  
```

\end{minipage}
\begin{minipage}{0.52\textwidth}

```{r NN table 2, include = TRUE, echo = FALSE, results = "asis"}
kbl(table6_NNvar[26:50,], row.names = FALSE, caption = "NN importance (cont.)", format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  column_spec(3, width = "12mm") %>% 
  column_spec(4:5, width = "1cm")
```
\end{minipage}

\newpage

### \large XGBoost Regression

For the XGBoost Regression models, the `caret` package uses the Gain method to calculate variable importance. Like the Neural Networks, categorical variables had to be one-hot encoded. The same method was applied to aggregate the importance. Results are listed below in tables 8 and 9.  

```{r Table 8 and 9 Preparation}
all_varImp_df <- read_csv("data/all_varImp_10C.csv")

# Round for legibility
all_varImp_df[, sapply(all_varImp_df, is.numeric)] <- lapply(all_varImp_df[, sapply(all_varImp_df, is.numeric)],
                                                                     function(x) round(x, 4))

# Group by Dependent and select the Variables with the highest ImportanceScaled scores
table8_XGvar <- all_varImp_df %>%
  filter(Model == "Adaptive Boosting") %>% 
  group_by(Dependent) %>%
  slice_max(ImportanceScaled, n = 5) %>%
  ungroup() %>%
  arrange(factor(Dependent, levels = dependent_names)) %>% 
  select(-Model)

# Change column name for consistency with other tables
colnames(table8_XGvar)[which(names(table8_XGvar) == "Dependent")] <- "Target"

# Add cluster number
table8_XGvar <- cbind(Cl. = rep(1:10, each = 5), table8_XGvar)
```

\begin{minipage}{0.52\textwidth}

```{r XG table 1, include = TRUE, echo = FALSE, results= "asis"}
colnames(table8_XGvar)[which(names(table8_XGvar) == "ImportanceScaled")] <- "Imp. Scaled"
colnames(table8_XGvar)[which(names(table8_XGvar) == "ImportanceRaw")] <- "Imp. Raw"

kbl(table8_XGvar[1:25,], format = "latex", caption = "XG importance", booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  column_spec(3, width = "12mm") %>% 
  column_spec(4:5, width = "1cm")
  
```

\end{minipage}
\begin{minipage}{0.52\textwidth}

```{r XG table 2, include = TRUE, echo = FALSE, results = "asis"}
kbl(table8_XGvar[26:50,], row.names = FALSE, caption = "XG importance (cont.)", format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  column_spec(3, width = "12mm") %>% 
  column_spec(4:5, width = "1cm")
```
\end{minipage}

\newpage

### \large Adaptive Boosting 

For the Adaptive Boosting models, the `caret` package uses the Gini method to calculate variable importance. Results are listed below in tables 10 and 11. 

```{r Table 10 and 11 Preparation}
all_varImp_df <- read_csv("data/all_varImp_10C.csv")

# Round for legibility
all_varImp_df[, sapply(all_varImp_df, is.numeric)] <- lapply(all_varImp_df[, sapply(all_varImp_df, is.numeric)],
                                                                     function(x) round(x, 4))

# Group by Dependent and select the Variables with the highest ImportanceScaled scores
table10_ABvar <- all_varImp_df %>%
  filter(Model == "XGBoost Regression") %>% 
  group_by(Dependent) %>%
  slice_max(ImportanceScaled, n = 5) %>%
  ungroup() %>%
  arrange(factor(Dependent, levels = dependent_names)) %>% 
  select(-Model)

# Change column name for consistency with other tables
colnames(table10_ABvar)[which(names(table10_ABvar) == "Dependent")] <- "Target"

# Add cluster number
table10_ABvar <- cbind(Cl. = rep(1:10, each = 5), table10_ABvar)
```

\begin{minipage}{0.52\textwidth}

```{r AB table 1, include = TRUE, echo = FALSE, results= "asis"}
colnames(table10_ABvar)[which(names(table10_ABvar) == "ImportanceScaled")] <- "Imp. Scaled"
colnames(table10_ABvar)[which(names(table10_ABvar) == "ImportanceRaw")] <- "Imp. Raw"

kbl(table10_ABvar[1:25,], format = "latex", caption = "AB importance", booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  column_spec(3, width = "12mm") %>% 
  column_spec(4:5, width = "1cm")
  
```

\end{minipage}
\begin{minipage}{0.52\textwidth}

```{r AB table 2, include = TRUE, echo = FALSE, results = "asis"}
kbl(table10_ABvar[26:50,], row.names = FALSE, caption = "AB importance (cont.)", format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  column_spec(3, width = "12mm") %>% 
  column_spec(4:5, width = "1cm")
```
\end{minipage}

\newpage

### \large Overall variable importance

For each dependent variable, the mean of the importance score was calculated across each algorithm, resulting in the overall most important variables. The Random Forest algorithm was excluded from this calculation, as performance was very poor on each dependent variable. The result is below in table 12 and 13.

```{r Table preparation}
library(dplyr)

all_varImp_df <- read_csv("data/all_varImp_10C.csv") 

table_totalVarImp <- all_varImp_df %>%
  filter(Model != "Random Forest") %>%
  group_by(Dependent, Variable) %>%
  summarize(Mean_ImpScaled = mean(ImportanceScaled)) %>%
  group_by(Dependent) %>%
  top_n(n = 5, wt = Mean_ImpScaled) %>%
  select(Dependent, Variable, Mean_ImpScaled) %>%
  arrange(factor(Dependent, levels = dependent_names), desc(Mean_ImpScaled))

# Change column name for consistency with other tables
colnames(table_totalVarImp)[which(names(table_totalVarImp) == "Dependent")] <- "Target"
colnames(table_totalVarImp)[which(names(table_totalVarImp) == "Mean_ImpScaled")] <- "Mean Imp. (scaled)"

# Add cluster number
table_totalVarImp  <- cbind(Cl. = rep(1:10, each = 5), table_totalVarImp)

# Round for legibility
table_totalVarImp[, sapply(table_totalVarImp, is.numeric)] <- lapply(table_totalVarImp[, sapply(table_totalVarImp, is.numeric)],
                                                                     function(x) round(x, 2))
```

\begin{minipage}{0.5\textwidth}

```{r overalltable 1, include = TRUE, echo = FALSE, results= "asis"}
kbl(table_totalVarImp[1:25,], format = "latex", caption = "Overall importance", booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  column_spec(4, width = "1cm")
```

\end{minipage}
\begin{minipage}{0.5\textwidth}

```{r overalltable 2, include = TRUE, echo = FALSE}
kbl(table_totalVarImp[26:50,], caption = "Overall importance (cont.)", format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position") %>% 
  column_spec(4, width = "1cm")
```
\end{minipage}

Since the raw variable names are hard to interpret, definitions of all important variables are given for each cluster. Explanations are not repeated for legibility. 

Cluster 1, which conveyed the missingness in the question *“How well off would you say your family/the people you live with are?”*, was associated with the variables `drinkloc`, `smcost`, `emotion1`, `hyper1`, and `prosoc1`. The variable `drinkloc` holds information about the number of locations where the adolescent drinks, `smcost` about how much the adolescent thinks a pack of cigarettes costs, `emotion1` about the emotional symptoms score of the adolescent, `hyper1` about the hyperactivity score of the adolescent, and `prosoc1` about prosocial behaviour.

Next was cluster 2, signifying the missingness in the multiple options to the question *“If you wanted information about drugs, who/where would you go to?”*, was associated with the variables `smcost`, `afraid`, `drinkloc`, `emotion1`, and `prosoc1`. The variable `afraid` holds information about the statement *“I have many fears, I am easily scared"*.

Then, cluster 3, which consisted of the missingness in questions about the subject *How much information school has provided the adolescent about alcohol/drugs, according to the adolescent*, was associated with the variables, `kind`, `sshares`, `prosoc1`, `schsm`, and `helpout`. The variable `kind` holds information about whether the adolescent feels like they are kind to younger children, `sshares` about whether the adolescent usually shares with others, `schsm` about whether the adolescent feels like their school has provided them with knowledge about smoking, and `helpout` about volunteering activities of the adolescent. 

Cluster 4, which consisted of the missingness in questions about the subject *How much advice / support school has provided the adolescent about alcohol/drugs, according to the adolescent*, was associated with the variables `sshares`,  `hpbclass`, `kind`, `drgdecs`, and `drglife`. The variable `hpbclass` holds information about whether the adolescent thinks the statement “Injecting drugs can lead to Hepatitis B” is true, `drgdecs` whether the adolescent believes the ability to make decisions can be affected by taking drugs, and `drglife` about whether they believe taking drugs has effect on other areas of their life. 

Next was cluster 5, which represented the missingness in questions about the subject *How much the adolescent is enjoying school*, was associated with the variables `skind`, `kind`, `drgdecs`, `prosoc1`, and `shares`. The variable `skind` holds information about whether the adolescent believes they are kind to others.

Cluster 6, which consisted of the missingness in the question *“In the past year, how many times did you skip or skive school?”*, was associated with the variables `drgdecs`, `schsm`, `hpbclass`, `whnlvskl`, and afraid. The variable ` whnlvskl` holds information about what the adolescent will likely be doing after they have finished school. 

Cluster 7 followed, which described the missingness in  the *Strengths & Difficulties* questionnaire, was associated with the variables `drglife`, `allife`, `smcost`, `avoiddrg`, and `drgrisk`. The variable `allife` holds information about whether they believe alcohol has effect on other areas of their life, `avoiddrg` about how confident the adolescent feels about being able to avoid drugs, and `drgrisk` about how much the adolescent has learned in school about the risks of drugs.

Next was cluster 8, which represented the missingness in questions about the subject *How much the parental figures know about them and their activies, according to the adolescent*, was associated with the variables `talkpa`, `drinkloc`, `famstat2`, `smkdad`, and `hyper1`. The variable `talkpa` holds information about the likelihood of the adolescent speaking to their father/carer if worried about something, `famstat2` about family status (if both parents are present in their life), and `smkdad` about whether their dad smokes.

Cluster 9, which consisted of the missingness in the question whether the adolescent does nothing in their free time, was associated with the variables `drgdrecs`, `afraid`, `ssomatic`, `numeffects`, and `hpcclass`. The variable `ssomatic` holds information about whether the adolescent regularly experiences headaches, stomach-aches or sickness.

Finally, cluster 10 consisted of the missingness in question “How much evenings the adolescent spends with friends in a typical week”, was associated with the variables `kind`, `schdrg`, `sex`, `schsm`, and `shelpout`. The variable `sex` is about the sex of the adolescent.

<!-- Discussion -->


# \LARGE Discussion

The project set out to determine whether variables were able to consistently predict item non-response in Scottish adolescents through multiverse modelling. Multiverse modelling was implemented by using four different algorithmic models, along with different tuned versions for each of those models. While the performance of the models was relevant, the individual variable importance was at the core of the project.

Overall, every algorithm except for the Random Forest performed decent, depending on the target variable. The Random Forest models had a poor performance on every target variable. Because of this reason, variables of those models were not taken into account for the overall variable importance. With the results, three notable discoveries were made: First, the variables that belonged to the *Strengths and Difficulties* questionnaire were associated with missingness in several dependent variables, since the variables `emotion1`, `afraid`, `prosoc1`, `hyper1`, and `sshares` are shown to be important variables in multiple clusters. The second notable finding is regarding cluster 7, which represented the missingness on the questionnaire itself. The five most important variables were all regarding alcohol and drugs. Lastly, missingness in `dadscore2` (cluster 8), which contained information about how much the adolescent thought their parents knew of them and their activities, was strongly associated with `talkpa`. This variable pertains to the likelihood of the adolescent speaking to their father/carer if they are worried about something, and was the most important association with missingness in `dadscore` for all four models.

A minor limitation of the project is the generalisibility of the results. Since the data only contains information about Scottish adolescents, the results might not apply to other age groups, or other nationalities for that matter. Referring to these results should thus not be done without discretion. A second limitation is related to the optimisation of the hyperparameters of the algorithmic models. The grid search was opted for since this is supported by the `caret` package in R, and requires manual implementation. However, this approach is limiting, as only a restricted amount of combinations of hyperparameters is tested. Other approaches to tune hyperparameters, like the Artifical Bee Colony algorithm (ABC), or genetic algorithms, search for optimal hyperparameters in a more complex manner, and may thus be the better option  (Karaboga, Gorkemli, Ozturk & Karaboga, 2014; Alibrahim & Ludwig, 2021). Additionally, the preprocessing steps could be perceived as a limitation. In hindsight, they were too harsh, which might have impeded the process. It would have been informing to run the analyses again with minimal preprocessing, and to examine whether results would have differed significantly. Although this could unfortunately not happen due to time constraints, it is certainly valuable to take into account for future research. A final limitation is the mediocre quality of the results. This have been caused by the class imbalance of the data. Notably, all of the baseline versions of the models experienced difficulty with this, as every one of them predicted all of the unseen data as ‘present’, essentially not being able to discern what could detect a value to be missing. Naturally, the balanced accuracies of these models were consistently circa 0.5.  

Furthermore, it should be emphasised that while the imbalance of the data may have been the cause (or one of the causes) of the mediocre results, this should not be interpreted as a warning to future endeavors. A considerable amount of public data in the social sciences, especially surveys stored in national databases (like the LISS panel or data of the UK Data Service), will typically not contain variables with more than 15% missingness. This data should not be viewed as less relevant, as overlooking this missingness could prevent insights about crucial variables from happening. A different recommendation is concerning the two tuning methods (class weights and threshold tuning) that were used in the project to account for the imbalanced data. These two are both algorithm-based, meaning they only tweak how the selected algorithms operate. Various other sampling methods, like oversampling or SMOTE, address the imbalance problem by focussing on the data instead. It would be worthwile for future research to also make use of these approaches, though implementing these could result in biased results if done without discretion (Vandewiele et al., 2021). As a final recommendation, researchers should not hesitate to opt for smaller data sets to analyse. While a large data set can allow for greater generalisibility and statistical power (Wolf, Harrington, Clark & Miller, 2013), it significantly slowed down the progress of this project, especially with the more complex algorithms (like Neural Networks and XGBoost Regression). Considering this was more of an exploratory analysis (which does not aim to establish causality), the project would have benefited from analysing a smaller amount of data. Especially when more variables might be examined as the dependent variable, it would be more efficient to analyse multiple different smaller data sets instead of one larger one. This is also more in line with the principles of multiverse analysis (Steegen et al., 2016). Moreover, a smaller data set would have also allowed for the sophisticated hyperparameter tuning practices mentioned earlier. 	

The closing point of discussion is on two ethical concerns. First is the possible harm of using machine learning algorithms, or rather, what amount of influence their results should be granted. If a certain variable can consistently predict missingness (in other variables) through exploratory analyses like this project, this does not automatically mean they hold any causal property. This is a common pitfall in data science, but domain knowledge is necessary to certify findings made with machine learning algorithms (Grimmer, 2015; Suresh & Guttag, 2019).  The second ethical consideration expands on the first one, and is regarding the possible consequences of the common knowledge base mentioned in the introduction, and the inferences made in research on these patterns in general. As research on these non-response patterns continues, certain conclusions could be taken out of context, which might slowly bring forth stigmas. To illustrate an exaggerated example, if a specific demographic is consistently shown to exhibit non-response, this could develop into a persistent association with this demographic. This association might in turn deter future researchers from making an effort to include this demographic. Since the goal is to scientifically find evidence for these patterns, the intention of this example is not to suggest that these findings should not be reported; the key is how they are reported. Ultimately, the point that should be emphasised is the importance of inclusive data collection, instead of facilitating singular associations that can easily be taken out of context. Of course, how people (or society as a whole) will interpret these results is out of the hands of the researchers to a certain extent. Nonetheless, the obligation of scientific research to communicate findings in a responsible manner should not cease to be reiterated. 

## \Large Conclusion

While the project was exploratory, it could be a significant step in determining what variables might be associated with variables regarding mental health, and alcohol and drug use among adolescents. A relevant discovery was made, given that the variables belonging to the mental health questionnaire were associated with missingness in several dependent variables. Future research could consider to include mental health constructs, and perform confirmatory analyses to determine whether they might belong to true response models of other variables.

Additionally, although its limitations were discussed earlier, machine learning is excellent for these types of projects, and necessary to enrich the common knowledge base. The practice of multiverse analysis is advised to ensure transparency and reliability of the findings. It is worth emphasising for future projects that if a variable cannot be predicted well by multiple algorithms, this could also lead to a meaningful finding. The variables that the algorithms were trained with are possibly not associated with missingness in that variable, which is relevant in uncovering the true response model as well. 

In conclusion,  missing data is ubiquitous, and should be addressed with the right methods in order to prevent harmful biases in the data. Biased data could lead to inaccurate conclusions, which (especially in the social sciences) could have detrimental consequences. Since knowledge on response mechanisms allows future projects to better implement these methods, research on them remains highly relevant.

<!-- References -->


\clearpage

# \LARGE References & Appendices

\
\begingroup
\noindent
\vspace{-2em}
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{5pt}

<div id="refs"></div>

Alibrahim, H., & Ludwig, S. A. (2021). Hyperparameter optimization: Comparing genetic algorithm against grid search and bayesian optimization. In 2021 IEEE Congress on Evolutionary Computation (CEC) (1551-1559).

Bell, B. A., Kromrey, J. D., & Ferron, J. M. (2009). Missing data and complex samples: The impact of listwise deletion vs. subpopulation analysis on statistical bias and hypothesis test results when data are MCAR and MAR. In Proceedings of the Joint Statistical Meetings, Survey Research Methods Section (Vol. 26, pp. 759-4770).

Berndt, M., Schmidt, F. M., Sailer, M., Fischer, F., Fischer, M. R., & Zottmann, J. M. (2021). Investigating statistical literacy and scientific reasoning & argumentation in medical-, social sciences-, and economics students. Learning and Individual Differences, 86, 101963.

Brown, I., & Mues, C. (2012). An experimental comparison of classification algorithms for imbalanced credit scoring data sets. Expert Systems with Applications, 39(3), 3446-3453.

Feng, D. C., Liu, Z. T., Wang, X. D., Chen, Y., Chang, J. Q., Wei, D. F., & Jiang, Z. M. (2020). Machine learning-based compressive strength prediction for concrete: An adaptive boosting approach. Construction and Building Materials, 230, 117000.

Gevrey, M., Dimopoulos, I., & Lek, S. (2003). Review and comparison of methods to study the contribution of variables in artificial neural network models. Ecological modelling, 160(3), 249-264.

Grimmer, J. (2015). We are all social scientists now: How big data, machine learning, and causal inference work together. PS: Political Science & Politics, 48(1), 80-83.

Grimmer, J., Roberts, M. E., & Stewart, B. M. (2021). Machine learning for social science: An agnostic approach. Annual Review of Political Science, 24, 395-419.

Gutierrez, A., & Sher, L. (2015). Alcohol and drug use among adolescents: an educational overview. International journal of adolescent medicine and health, 27(2), 207-212.

Ipsos MORI Scotland. (2020). Scottish Schools Adolescent Lifestyle and Substance Use Survey, 2018. [data collection]. UK Data Service. SN: 8615, http://doi.org/10.5255/UKDA-SN-8615-1

Karaboga, D., Gorkemli, B., Ozturk, C., & Karaboga, N. (2014). A comprehensive survey: artificial bee colony (ABC) algorithm and applications. Artificial Intelligence Review, 42, 21-57.

Kieling, C., Baker-Henningham, H., Belfer, M., Conti, G., Ertem, I., Omigbodun, O., Rohde, L.A., Srinath, S., Ulkuer, N. & Rahman, A. (2011). Child and adolescent mental health worldwide: evidence for action. The Lancet, 378(9801), 1515-1525.

Kuhn, M., Wing, J., Weston, S., Williams, A., Keefer, C., Engelhardt, A., Cooper, T., Mayer, Z., Kenkel, B., 
Team, R.C. (2020). Package ‘caret’. The R Journal, 223(7).

Lipton, Z. C., Berkowitz, J., & Elkan, C. (2015). A critical review of recurrent neural networks for sequence learning. arXiv preprint arXiv:1506.00019.

Liss Data. (2022). Access Data. Retrieved May 25, 2023, from https://www.lissdata.nl/

Margineantu, D. D., & Dietterich, T. G. (1997, July).  Pruning adaptive boosting.  In ICML (Vol.  97, pp.	211-218). 

Pan, W., & Tang, M. (2004). Examining the effectiveness of innovative instructional methods on reducing statistics anxiety for graduate students in the social sciences. Journal of Instructional Psychology, 31(2).

Pepinsky, T. B. (2018). A note on listwise deletion versus multiple imputation. Political Analysis, 26(4), 480-488. 

Ramraj, S., Uzir, N., Sunil, R., & Banerjee, S. (2016). Experimenting XGBoost algorithm for prediction and classification of different datasets. International Journal of Control Theory and Applications, 9(40), 651-662.

Särndal, C. E., & Lundström, S. (2005). Estimation in surveys with nonresponse. John Wiley & Sons.

Savage, C., Hübner, N., Biewen, M., Nagengast, B., & Polikoff, M. S. (2021). Social studies textbook effects: Evidence from Texas. Aera Open, 7, 2332858421992345.

Smith, G. D., & Ebrahim, S. (2002). Data dredging, bias, or confounding: They can all get you into the BMJ and the Friday papers. Bmj, 325(7378), 1437-1438.

Speiser, J. L., Miller, M. E., Tooze, J., & Ip, E. (2019). A comparison of random forest variable selection methods for classification prediction modeling. Expert systems with applications, 134, 93-101.

Steegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science, 11(5), 702-712.

Suresh, H., & Guttag, J. V. (2019). A framework for understanding unintended consequences of machine learning. arXiv preprint arXiv:1901.10002, 2(8).

Van Buuren, S. (2018). Flexible imputation of missing data. CRC press.

Van Buuren, S., & Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputation by chained equations in R. Journal of statistical software, 45, 1-67.

Vandewiele, G., Dehaene, I., Kovács, G., Sterckx, L., Janssens, O., Ongenae, F., De Backere, F., De Turck, F., Roelens, K., Decruyenaere, J., Van Hoecke, S., Demeester, T. (2021). Overly optimistic prediction results on imbalanced data: a case study of flaws and benefits when applying over-sampling. Artificial Intelligence in Medicine, 111, 101987.

Vicente-Saez, R., & Martinez-Fuentes, C. (2018). Open Science now: A systematic literature review for an integrated definition. Journal of business research, 88, 428-436.

Witte, J., Foraita, R., & Didelez, V. (2022). Multiple imputation and test‐wise deletion for causal discovery with incomplete cohort data. Statistics in Medicine, 41(23), 4716-4743.

Wulff, J. N., & Jeppesen, L. E. (2017). Multiple imputation by chained equations in praxis: guidelines and review. Electronic Journal of Business Research Methods, 15(1), 41-56.

Wolf, E. J., Harrington, K. M., Clark, S. L., & Miller, M. W. (2013). Sample size requirements for structural equation models: An evaluation of power, bias, and solution propriety. Educational and psychological measurement, 73(6), 913-934. 


\endgroup


<!-- Appendices -->


\clearpage

## \large Appendix A

Appendix A contains information about the data used in the project, and where it can be retrieved.

The data was provided by the UK Data Service, of which the general site can be found with this link: https://ukdataservice.ac.uk/

To gain access to the data, an e-mail has to be sent to the UK Data Service stating your intentions with the data, and which institute you belong to. Note that if the goal of the project is commercial, the UK Data Service may decide to refrain from sharing the data. The link to the specific data set can be found here: http://doi.org/10.5255/UKDA-SN-8615-1 (This reference is also included in the reference list)

\clearpage

## \large Appendix B

Appendix B contains information about the model performance of all the versions of each algorithms. 

```{r Table 6, include = TRUE, echo = FALSE, fig.align = "center", results = "asis"}

kbl(all_results_df, format = "latex", longtable = TRUE, caption = "All results", booktabs = TRUE) %>% 
  kable_styling(latex_options = c("striped", "repeat_header")) %>% 
  column_spec(1:2, width = "2cm", latex_valign = "m") %>%
  column_spec(3, width = "3cm", latex_valign = "m") %>%
  column_spec(4:8, width = "1cm", latex_valign = "m")
```






